{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Data\n",
    "\n",
    "## Environment Settings\n",
    "\n",
    "An statistical Analysis of the data captured will be performed.\n",
    "\n",
    "The environment configuration is the following:\n",
    "\n",
    "- A rectangle area is used whose dimension is 2 x 1.5 meters. \n",
    "- A custom robot similar to an epuck was used.\n",
    "- The robot starts in the middle of the arena.\n",
    "- The robot moves in a random fashion way around the environment avoiding obstacles.\n",
    "- The robot has 8 sensors that measure the distance between the robot and the walls.\n",
    "- Some noise was introduced in the sensors measurements of the robot using the concept of [lookup tables](https://cyberbotics.com/doc/reference/distancesensor) in the Webots simulator which according to Webots documentation \"The first column of the table specifies the input distances, the second column specifies the corresponding desired response values, and the third column indicates the desired standard deviation of the noise. The noise on the return value is computed according to a gaussian random number distribution whose range is calculated as a percent of the response value (two times the standard deviation is often referred to as the signal quality)\". The following values were taken:\n",
    "\n",
    "    -First experiment:\n",
    "        - (0, 0, 0.01)\n",
    "        - (10, 10, 0.01)\n",
    "    -Second experiment:\n",
    "    \n",
    "        - (0, 0, 0.2)\n",
    "        - (10, 10, 0.2)\n",
    "- The simulator runs during 10 minutes in fast mode which is translated into 12 hours of collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (0.22)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from keras) (5.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/site-packages (from keras) (1.16.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages (from keras) (2.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.920614</td>\n",
       "      <td>0.761198</td>\n",
       "      <td>168.209483</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>-11.790739</td>\n",
       "      <td>1.085179</td>\n",
       "      <td>0.790267</td>\n",
       "      <td>0.893342</td>\n",
       "      <td>...</td>\n",
       "      <td>1.139790</td>\n",
       "      <td>1.144901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.850135</td>\n",
       "      <td>0.775909</td>\n",
       "      <td>168.212418</td>\n",
       "      <td>-0.070479</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.571635</td>\n",
       "      <td>0.596799</td>\n",
       "      <td>0.883340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>1.028332</td>\n",
       "      <td>-0.513544</td>\n",
       "      <td>-0.193468</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-0.070277</td>\n",
       "      <td>-0.387726</td>\n",
       "      <td>-0.309733</td>\n",
       "      <td>-0.116568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.779657</td>\n",
       "      <td>0.790625</td>\n",
       "      <td>168.209551</td>\n",
       "      <td>-0.070478</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.581452</td>\n",
       "      <td>0.904627</td>\n",
       "      <td>0.689004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.889130</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.307828</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>0.239518</td>\n",
       "      <td>0.206480</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>-0.338857</td>\n",
       "      <td>-0.139203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.709174</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>168.212871</td>\n",
       "      <td>-0.070483</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.956302</td>\n",
       "      <td>0.842911</td>\n",
       "      <td>0.796714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246415</td>\n",
       "      <td>0.712158</td>\n",
       "      <td>0.374849</td>\n",
       "      <td>-0.061716</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>-0.345782</td>\n",
       "      <td>-0.084918</td>\n",
       "      <td>0.755215</td>\n",
       "      <td>-0.176971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.638698</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>168.208857</td>\n",
       "      <td>-0.070477</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>0.671731</td>\n",
       "      <td>0.779896</td>\n",
       "      <td>0.962191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>0.595164</td>\n",
       "      <td>-0.284570</td>\n",
       "      <td>-0.063014</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.128150</td>\n",
       "      <td>-0.054777</td>\n",
       "      <td>-0.678608</td>\n",
       "      <td>-0.116994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         x         y       theta        dx        dy     dtheta  \\\n",
       "0           0  0.920614  0.761198  168.209483 -0.070670  0.011198 -11.790739   \n",
       "1           1  0.850135  0.775909  168.212418 -0.070479  0.014711   0.002935   \n",
       "2           2  0.779657  0.790625  168.209551 -0.070478  0.014716  -0.002867   \n",
       "3           3  0.709174  0.805340  168.212871 -0.070483  0.014715   0.003319   \n",
       "4           4  0.638698  0.820056  168.208857 -0.070477  0.014716  -0.004013   \n",
       "\n",
       "   sensor_1  sensor_2  sensor_3    ...      sensor_7  sensor_8  dsensor_1  \\\n",
       "0  1.085179  0.790267  0.893342    ...      1.139790  1.144901        NaN   \n",
       "1  0.571635  0.596799  0.883340    ...      0.830057  1.028332  -0.513544   \n",
       "2  0.581452  0.904627  0.689004    ...      0.491200  0.889130   0.009817   \n",
       "3  0.956302  0.842911  0.796714    ...      1.246415  0.712158   0.374849   \n",
       "4  0.671731  0.779896  0.962191    ...      0.567806  0.595164  -0.284570   \n",
       "\n",
       "   dsensor_2  dsensor_3  dsensor_4  dsensor_5  dsensor_6  dsensor_7  dsensor_8  \n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "1  -0.193468  -0.010002  -0.430864  -0.070277  -0.387726  -0.309733  -0.116568  \n",
       "2   0.307828  -0.194336   0.239518   0.206480   0.293382  -0.338857  -0.139203  \n",
       "3  -0.061716   0.107710   0.075412  -0.345782  -0.084918   0.755215  -0.176971  \n",
       "4  -0.063014   0.165477   0.005216   0.128150  -0.054777  -0.678608  -0.116994  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = 'robot_info_dataset-jumped.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected 1384848 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65342, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains some null values so they should be deleted from the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.850135</td>\n",
       "      <td>0.775909</td>\n",
       "      <td>168.212418</td>\n",
       "      <td>-0.070479</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.571635</td>\n",
       "      <td>0.596799</td>\n",
       "      <td>0.883340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>1.028332</td>\n",
       "      <td>-0.513544</td>\n",
       "      <td>-0.193468</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-0.070277</td>\n",
       "      <td>-0.387726</td>\n",
       "      <td>-0.309733</td>\n",
       "      <td>-0.116568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.779657</td>\n",
       "      <td>0.790625</td>\n",
       "      <td>168.209551</td>\n",
       "      <td>-0.070478</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.581452</td>\n",
       "      <td>0.904627</td>\n",
       "      <td>0.689004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.889130</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.307828</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>0.239518</td>\n",
       "      <td>0.206480</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>-0.338857</td>\n",
       "      <td>-0.139203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.709174</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>168.212871</td>\n",
       "      <td>-0.070483</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.956302</td>\n",
       "      <td>0.842911</td>\n",
       "      <td>0.796714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246415</td>\n",
       "      <td>0.712158</td>\n",
       "      <td>0.374849</td>\n",
       "      <td>-0.061716</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>-0.345782</td>\n",
       "      <td>-0.084918</td>\n",
       "      <td>0.755215</td>\n",
       "      <td>-0.176971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.638698</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>168.208857</td>\n",
       "      <td>-0.070477</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>0.671731</td>\n",
       "      <td>0.779896</td>\n",
       "      <td>0.962191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>0.595164</td>\n",
       "      <td>-0.284570</td>\n",
       "      <td>-0.063014</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.128150</td>\n",
       "      <td>-0.054777</td>\n",
       "      <td>-0.678608</td>\n",
       "      <td>-0.116994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.568214</td>\n",
       "      <td>0.834773</td>\n",
       "      <td>168.212811</td>\n",
       "      <td>-0.070484</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.751139</td>\n",
       "      <td>0.495261</td>\n",
       "      <td>0.742213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575172</td>\n",
       "      <td>0.523609</td>\n",
       "      <td>0.079408</td>\n",
       "      <td>-0.284635</td>\n",
       "      <td>-0.219978</td>\n",
       "      <td>-0.447768</td>\n",
       "      <td>0.549075</td>\n",
       "      <td>0.118147</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>-0.071555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.499522</td>\n",
       "      <td>0.854553</td>\n",
       "      <td>134.137941</td>\n",
       "      <td>-0.068692</td>\n",
       "      <td>0.019781</td>\n",
       "      <td>-34.074870</td>\n",
       "      <td>0.736352</td>\n",
       "      <td>0.588845</td>\n",
       "      <td>1.498086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485124</td>\n",
       "      <td>0.411577</td>\n",
       "      <td>-0.014787</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.755874</td>\n",
       "      <td>0.881613</td>\n",
       "      <td>-0.946182</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>-0.090048</td>\n",
       "      <td>-0.112032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.471813</td>\n",
       "      <td>0.920420</td>\n",
       "      <td>111.074435</td>\n",
       "      <td>-0.027709</td>\n",
       "      <td>0.065866</td>\n",
       "      <td>-23.063507</td>\n",
       "      <td>0.419531</td>\n",
       "      <td>1.069214</td>\n",
       "      <td>1.768725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310053</td>\n",
       "      <td>0.803526</td>\n",
       "      <td>-0.316821</td>\n",
       "      <td>0.480369</td>\n",
       "      <td>0.270639</td>\n",
       "      <td>-0.734570</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>-0.706186</td>\n",
       "      <td>-0.175071</td>\n",
       "      <td>0.391949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.445931</td>\n",
       "      <td>0.987610</td>\n",
       "      <td>111.074646</td>\n",
       "      <td>-0.025882</td>\n",
       "      <td>0.067190</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.536241</td>\n",
       "      <td>0.483614</td>\n",
       "      <td>1.371695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405266</td>\n",
       "      <td>0.604981</td>\n",
       "      <td>0.116710</td>\n",
       "      <td>-0.585600</td>\n",
       "      <td>-0.397030</td>\n",
       "      <td>-0.102371</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>-0.056112</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>-0.198545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.418390</td>\n",
       "      <td>1.054107</td>\n",
       "      <td>114.933738</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.066498</td>\n",
       "      <td>3.859092</td>\n",
       "      <td>0.342284</td>\n",
       "      <td>0.720183</td>\n",
       "      <td>1.895051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428127</td>\n",
       "      <td>0.524840</td>\n",
       "      <td>-0.193957</td>\n",
       "      <td>0.236569</td>\n",
       "      <td>0.523355</td>\n",
       "      <td>1.116081</td>\n",
       "      <td>0.363247</td>\n",
       "      <td>-0.000967</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>-0.080141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.394894</td>\n",
       "      <td>1.122021</td>\n",
       "      <td>107.231961</td>\n",
       "      <td>-0.023496</td>\n",
       "      <td>0.067914</td>\n",
       "      <td>-7.701777</td>\n",
       "      <td>0.244309</td>\n",
       "      <td>0.488843</td>\n",
       "      <td>1.385274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421095</td>\n",
       "      <td>0.502860</td>\n",
       "      <td>-0.097975</td>\n",
       "      <td>-0.231340</td>\n",
       "      <td>-0.509776</td>\n",
       "      <td>-0.739754</td>\n",
       "      <td>-0.281965</td>\n",
       "      <td>-0.093100</td>\n",
       "      <td>-0.007032</td>\n",
       "      <td>-0.021980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.373575</td>\n",
       "      <td>1.190786</td>\n",
       "      <td>107.229023</td>\n",
       "      <td>-0.021319</td>\n",
       "      <td>0.068765</td>\n",
       "      <td>-0.002939</td>\n",
       "      <td>0.295913</td>\n",
       "      <td>0.457004</td>\n",
       "      <td>1.759191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271917</td>\n",
       "      <td>0.375203</td>\n",
       "      <td>0.051604</td>\n",
       "      <td>-0.031839</td>\n",
       "      <td>0.373917</td>\n",
       "      <td>-0.190467</td>\n",
       "      <td>0.580860</td>\n",
       "      <td>0.144419</td>\n",
       "      <td>-0.149179</td>\n",
       "      <td>-0.127657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.353191</td>\n",
       "      <td>1.259852</td>\n",
       "      <td>103.356119</td>\n",
       "      <td>-0.020384</td>\n",
       "      <td>0.069066</td>\n",
       "      <td>-3.872904</td>\n",
       "      <td>0.133681</td>\n",
       "      <td>0.255816</td>\n",
       "      <td>2.019363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374215</td>\n",
       "      <td>0.264384</td>\n",
       "      <td>-0.162232</td>\n",
       "      <td>-0.201188</td>\n",
       "      <td>0.260172</td>\n",
       "      <td>0.862895</td>\n",
       "      <td>-0.851819</td>\n",
       "      <td>-0.154971</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>-0.110818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.321467</td>\n",
       "      <td>1.322560</td>\n",
       "      <td>154.129909</td>\n",
       "      <td>-0.031724</td>\n",
       "      <td>0.062708</td>\n",
       "      <td>50.773790</td>\n",
       "      <td>0.198410</td>\n",
       "      <td>0.107779</td>\n",
       "      <td>0.223307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429172</td>\n",
       "      <td>0.260524</td>\n",
       "      <td>0.064729</td>\n",
       "      <td>-0.148038</td>\n",
       "      <td>-1.796056</td>\n",
       "      <td>-0.521964</td>\n",
       "      <td>0.646311</td>\n",
       "      <td>1.244288</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>-0.003861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.254388</td>\n",
       "      <td>1.310835</td>\n",
       "      <td>205.548630</td>\n",
       "      <td>-0.067079</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>51.418721</td>\n",
       "      <td>0.154634</td>\n",
       "      <td>0.271346</td>\n",
       "      <td>0.138330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.173755</td>\n",
       "      <td>0.389340</td>\n",
       "      <td>-0.043776</td>\n",
       "      <td>0.163567</td>\n",
       "      <td>-0.084977</td>\n",
       "      <td>-1.270904</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>-0.022878</td>\n",
       "      <td>0.744584</td>\n",
       "      <td>0.128816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.188872</td>\n",
       "      <td>1.283017</td>\n",
       "      <td>192.331127</td>\n",
       "      <td>-0.065516</td>\n",
       "      <td>-0.027818</td>\n",
       "      <td>-13.217502</td>\n",
       "      <td>0.169631</td>\n",
       "      <td>0.233973</td>\n",
       "      <td>0.175622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.432796</td>\n",
       "      <td>0.197966</td>\n",
       "      <td>0.014998</td>\n",
       "      <td>-0.037373</td>\n",
       "      <td>0.037292</td>\n",
       "      <td>0.108587</td>\n",
       "      <td>0.541179</td>\n",
       "      <td>0.215615</td>\n",
       "      <td>0.259041</td>\n",
       "      <td>-0.191374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.144871</td>\n",
       "      <td>1.319933</td>\n",
       "      <td>78.532227</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.036916</td>\n",
       "      <td>-113.798900</td>\n",
       "      <td>0.197391</td>\n",
       "      <td>0.833326</td>\n",
       "      <td>2.133678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110615</td>\n",
       "      <td>0.080214</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>0.599354</td>\n",
       "      <td>1.958056</td>\n",
       "      <td>1.054939</td>\n",
       "      <td>-1.634324</td>\n",
       "      <td>-1.665918</td>\n",
       "      <td>-1.322181</td>\n",
       "      <td>-0.117752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.199346</td>\n",
       "      <td>1.328425</td>\n",
       "      <td>295.983981</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>217.451754</td>\n",
       "      <td>1.446887</td>\n",
       "      <td>0.192367</td>\n",
       "      <td>0.161671</td>\n",
       "      <td>...</td>\n",
       "      <td>2.085222</td>\n",
       "      <td>0.942276</td>\n",
       "      <td>1.249496</td>\n",
       "      <td>-0.640960</td>\n",
       "      <td>-1.972006</td>\n",
       "      <td>-1.135311</td>\n",
       "      <td>-0.091708</td>\n",
       "      <td>0.088676</td>\n",
       "      <td>1.974607</td>\n",
       "      <td>0.862061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.223847</td>\n",
       "      <td>1.261762</td>\n",
       "      <td>285.663108</td>\n",
       "      <td>0.024501</td>\n",
       "      <td>-0.066664</td>\n",
       "      <td>-10.320873</td>\n",
       "      <td>1.057248</td>\n",
       "      <td>0.245830</td>\n",
       "      <td>0.154252</td>\n",
       "      <td>...</td>\n",
       "      <td>1.996760</td>\n",
       "      <td>1.442333</td>\n",
       "      <td>-0.389640</td>\n",
       "      <td>0.053463</td>\n",
       "      <td>-0.007419</td>\n",
       "      <td>0.087801</td>\n",
       "      <td>0.063392</td>\n",
       "      <td>0.129924</td>\n",
       "      <td>-0.088462</td>\n",
       "      <td>0.500058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.243284</td>\n",
       "      <td>1.192453</td>\n",
       "      <td>285.672356</td>\n",
       "      <td>0.019437</td>\n",
       "      <td>-0.069309</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>1.093535</td>\n",
       "      <td>0.292819</td>\n",
       "      <td>0.226315</td>\n",
       "      <td>...</td>\n",
       "      <td>1.907153</td>\n",
       "      <td>1.584347</td>\n",
       "      <td>0.036288</td>\n",
       "      <td>0.046989</td>\n",
       "      <td>0.072063</td>\n",
       "      <td>0.037655</td>\n",
       "      <td>0.078275</td>\n",
       "      <td>0.197717</td>\n",
       "      <td>-0.089606</td>\n",
       "      <td>0.142014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.262726</td>\n",
       "      <td>1.123126</td>\n",
       "      <td>285.673771</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>-0.069327</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>1.122716</td>\n",
       "      <td>0.326370</td>\n",
       "      <td>0.281348</td>\n",
       "      <td>...</td>\n",
       "      <td>2.113977</td>\n",
       "      <td>1.741571</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>0.033551</td>\n",
       "      <td>0.055033</td>\n",
       "      <td>0.110055</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.151624</td>\n",
       "      <td>0.206824</td>\n",
       "      <td>0.157224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.282172</td>\n",
       "      <td>1.053802</td>\n",
       "      <td>285.673408</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>-0.069323</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>1.106262</td>\n",
       "      <td>0.268065</td>\n",
       "      <td>0.208852</td>\n",
       "      <td>...</td>\n",
       "      <td>1.793466</td>\n",
       "      <td>1.437905</td>\n",
       "      <td>-0.016454</td>\n",
       "      <td>-0.058305</td>\n",
       "      <td>-0.072496</td>\n",
       "      <td>0.035736</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.174463</td>\n",
       "      <td>-0.320511</td>\n",
       "      <td>-0.303666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.301613</td>\n",
       "      <td>0.984477</td>\n",
       "      <td>285.672277</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>-0.069325</td>\n",
       "      <td>-0.001131</td>\n",
       "      <td>0.700907</td>\n",
       "      <td>0.415374</td>\n",
       "      <td>0.257344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.617733</td>\n",
       "      <td>1.353854</td>\n",
       "      <td>-0.405354</td>\n",
       "      <td>0.147309</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.182783</td>\n",
       "      <td>0.116906</td>\n",
       "      <td>-0.175733</td>\n",
       "      <td>-0.084051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.317117</td>\n",
       "      <td>0.914189</td>\n",
       "      <td>281.826766</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>-0.070288</td>\n",
       "      <td>-3.845511</td>\n",
       "      <td>1.053929</td>\n",
       "      <td>0.329018</td>\n",
       "      <td>0.295675</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047159</td>\n",
       "      <td>0.601494</td>\n",
       "      <td>0.353021</td>\n",
       "      <td>-0.086356</td>\n",
       "      <td>0.038331</td>\n",
       "      <td>-0.022708</td>\n",
       "      <td>-0.026225</td>\n",
       "      <td>-0.137646</td>\n",
       "      <td>-0.570574</td>\n",
       "      <td>-0.752361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.331865</td>\n",
       "      <td>0.843719</td>\n",
       "      <td>281.823130</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>-0.070471</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>0.689328</td>\n",
       "      <td>0.333630</td>\n",
       "      <td>0.177559</td>\n",
       "      <td>...</td>\n",
       "      <td>1.867602</td>\n",
       "      <td>1.049892</td>\n",
       "      <td>-0.364600</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>-0.118116</td>\n",
       "      <td>-0.010178</td>\n",
       "      <td>0.422480</td>\n",
       "      <td>0.247598</td>\n",
       "      <td>0.820444</td>\n",
       "      <td>0.448398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.346615</td>\n",
       "      <td>0.773243</td>\n",
       "      <td>281.827439</td>\n",
       "      <td>0.014750</td>\n",
       "      <td>-0.070476</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.729204</td>\n",
       "      <td>0.450479</td>\n",
       "      <td>0.301668</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143278</td>\n",
       "      <td>0.952727</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>0.116849</td>\n",
       "      <td>0.124109</td>\n",
       "      <td>0.204662</td>\n",
       "      <td>-0.259174</td>\n",
       "      <td>-0.262489</td>\n",
       "      <td>-0.724324</td>\n",
       "      <td>-0.097165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.361362</td>\n",
       "      <td>0.702773</td>\n",
       "      <td>281.822447</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>-0.070470</td>\n",
       "      <td>-0.004992</td>\n",
       "      <td>0.740336</td>\n",
       "      <td>0.423206</td>\n",
       "      <td>0.385625</td>\n",
       "      <td>...</td>\n",
       "      <td>1.824108</td>\n",
       "      <td>0.970899</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>0.083958</td>\n",
       "      <td>0.077142</td>\n",
       "      <td>0.515720</td>\n",
       "      <td>0.722845</td>\n",
       "      <td>0.680829</td>\n",
       "      <td>0.018172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.376111</td>\n",
       "      <td>0.632296</td>\n",
       "      <td>281.827300</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>-0.070477</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.513066</td>\n",
       "      <td>0.419759</td>\n",
       "      <td>0.362323</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943009</td>\n",
       "      <td>0.735944</td>\n",
       "      <td>-0.227270</td>\n",
       "      <td>-0.003446</td>\n",
       "      <td>-0.023302</td>\n",
       "      <td>-0.235438</td>\n",
       "      <td>-0.250889</td>\n",
       "      <td>0.312916</td>\n",
       "      <td>0.118902</td>\n",
       "      <td>-0.234954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.390860</td>\n",
       "      <td>0.561827</td>\n",
       "      <td>281.822443</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>-0.070469</td>\n",
       "      <td>-0.004858</td>\n",
       "      <td>0.280831</td>\n",
       "      <td>0.524966</td>\n",
       "      <td>0.453390</td>\n",
       "      <td>...</td>\n",
       "      <td>1.671884</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>-0.232236</td>\n",
       "      <td>0.105206</td>\n",
       "      <td>0.091066</td>\n",
       "      <td>0.157780</td>\n",
       "      <td>0.052931</td>\n",
       "      <td>-0.270492</td>\n",
       "      <td>-0.271126</td>\n",
       "      <td>-0.078049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.405607</td>\n",
       "      <td>0.491350</td>\n",
       "      <td>281.826488</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>-0.070477</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.379643</td>\n",
       "      <td>0.529145</td>\n",
       "      <td>0.447030</td>\n",
       "      <td>...</td>\n",
       "      <td>1.305905</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.098813</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>-0.006359</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>-0.002099</td>\n",
       "      <td>0.203375</td>\n",
       "      <td>-0.365979</td>\n",
       "      <td>-0.206628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.365783</td>\n",
       "      <td>0.451031</td>\n",
       "      <td>166.530722</td>\n",
       "      <td>-0.039825</td>\n",
       "      <td>-0.040319</td>\n",
       "      <td>-115.295766</td>\n",
       "      <td>0.252419</td>\n",
       "      <td>1.087424</td>\n",
       "      <td>1.342140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549343</td>\n",
       "      <td>0.318731</td>\n",
       "      <td>-0.127225</td>\n",
       "      <td>0.558279</td>\n",
       "      <td>0.895110</td>\n",
       "      <td>0.680145</td>\n",
       "      <td>-0.279006</td>\n",
       "      <td>-1.400963</td>\n",
       "      <td>-0.756562</td>\n",
       "      <td>-0.132536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65312</th>\n",
       "      <td>65312</td>\n",
       "      <td>1.506068</td>\n",
       "      <td>0.983709</td>\n",
       "      <td>286.798226</td>\n",
       "      <td>-0.040986</td>\n",
       "      <td>-0.043939</td>\n",
       "      <td>96.847750</td>\n",
       "      <td>0.889909</td>\n",
       "      <td>1.174579</td>\n",
       "      <td>1.463482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463487</td>\n",
       "      <td>0.895936</td>\n",
       "      <td>-0.397189</td>\n",
       "      <td>0.402950</td>\n",
       "      <td>0.940960</td>\n",
       "      <td>-0.017752</td>\n",
       "      <td>0.188211</td>\n",
       "      <td>-0.035631</td>\n",
       "      <td>-0.353121</td>\n",
       "      <td>-1.419340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65313</th>\n",
       "      <td>65313</td>\n",
       "      <td>1.525680</td>\n",
       "      <td>0.914969</td>\n",
       "      <td>286.169047</td>\n",
       "      <td>0.019612</td>\n",
       "      <td>-0.068740</td>\n",
       "      <td>-0.629179</td>\n",
       "      <td>0.866091</td>\n",
       "      <td>1.797614</td>\n",
       "      <td>1.762390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461549</td>\n",
       "      <td>0.551110</td>\n",
       "      <td>-0.023818</td>\n",
       "      <td>0.623036</td>\n",
       "      <td>0.298908</td>\n",
       "      <td>0.476274</td>\n",
       "      <td>-0.085996</td>\n",
       "      <td>-0.267685</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>-0.344826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65314</th>\n",
       "      <td>65314</td>\n",
       "      <td>1.545727</td>\n",
       "      <td>0.845812</td>\n",
       "      <td>286.173240</td>\n",
       "      <td>0.020047</td>\n",
       "      <td>-0.069157</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.685343</td>\n",
       "      <td>1.273496</td>\n",
       "      <td>1.331721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363549</td>\n",
       "      <td>0.698678</td>\n",
       "      <td>-0.180747</td>\n",
       "      <td>-0.524118</td>\n",
       "      <td>-0.430669</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.197342</td>\n",
       "      <td>0.101272</td>\n",
       "      <td>-0.098000</td>\n",
       "      <td>0.147568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65315</th>\n",
       "      <td>65315</td>\n",
       "      <td>1.565770</td>\n",
       "      <td>0.776662</td>\n",
       "      <td>286.168788</td>\n",
       "      <td>0.020044</td>\n",
       "      <td>-0.069150</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>1.996541</td>\n",
       "      <td>1.471416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427246</td>\n",
       "      <td>0.484253</td>\n",
       "      <td>0.126085</td>\n",
       "      <td>0.723045</td>\n",
       "      <td>0.139695</td>\n",
       "      <td>0.123192</td>\n",
       "      <td>-0.125717</td>\n",
       "      <td>-0.027803</td>\n",
       "      <td>0.063697</td>\n",
       "      <td>-0.214426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65316</th>\n",
       "      <td>65316</td>\n",
       "      <td>1.585816</td>\n",
       "      <td>0.707505</td>\n",
       "      <td>286.172872</td>\n",
       "      <td>0.020045</td>\n",
       "      <td>-0.069157</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.761846</td>\n",
       "      <td>0.875321</td>\n",
       "      <td>1.677064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.705448</td>\n",
       "      <td>-0.049582</td>\n",
       "      <td>-1.121219</td>\n",
       "      <td>0.205648</td>\n",
       "      <td>0.394226</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>0.070358</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.221195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65317</th>\n",
       "      <td>65317</td>\n",
       "      <td>1.637710</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>46.306106</td>\n",
       "      <td>0.051895</td>\n",
       "      <td>-0.024212</td>\n",
       "      <td>-239.866767</td>\n",
       "      <td>0.387084</td>\n",
       "      <td>0.370887</td>\n",
       "      <td>0.530013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003578</td>\n",
       "      <td>1.061082</td>\n",
       "      <td>-0.374762</td>\n",
       "      <td>-0.504434</td>\n",
       "      <td>-1.147051</td>\n",
       "      <td>-0.771560</td>\n",
       "      <td>0.591829</td>\n",
       "      <td>1.059905</td>\n",
       "      <td>0.551805</td>\n",
       "      <td>0.355633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65318</th>\n",
       "      <td>65318</td>\n",
       "      <td>1.681473</td>\n",
       "      <td>0.739371</td>\n",
       "      <td>53.595364</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.056078</td>\n",
       "      <td>7.289258</td>\n",
       "      <td>0.283493</td>\n",
       "      <td>0.257740</td>\n",
       "      <td>0.481433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937412</td>\n",
       "      <td>0.780959</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>-0.113147</td>\n",
       "      <td>-0.048580</td>\n",
       "      <td>0.065553</td>\n",
       "      <td>-0.499304</td>\n",
       "      <td>-0.140265</td>\n",
       "      <td>-0.066165</td>\n",
       "      <td>-0.280123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65319</th>\n",
       "      <td>65319</td>\n",
       "      <td>1.724195</td>\n",
       "      <td>0.797321</td>\n",
       "      <td>53.591446</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>-0.003919</td>\n",
       "      <td>0.242045</td>\n",
       "      <td>0.241740</td>\n",
       "      <td>0.414211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726876</td>\n",
       "      <td>0.605587</td>\n",
       "      <td>-0.041448</td>\n",
       "      <td>-0.016000</td>\n",
       "      <td>-0.067222</td>\n",
       "      <td>0.048685</td>\n",
       "      <td>0.056658</td>\n",
       "      <td>-0.760184</td>\n",
       "      <td>-0.210536</td>\n",
       "      <td>-0.175372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65320</th>\n",
       "      <td>65320</td>\n",
       "      <td>1.761483</td>\n",
       "      <td>0.858592</td>\n",
       "      <td>69.066101</td>\n",
       "      <td>0.037287</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>15.474655</td>\n",
       "      <td>0.275775</td>\n",
       "      <td>0.223374</td>\n",
       "      <td>0.363474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749277</td>\n",
       "      <td>0.447364</td>\n",
       "      <td>0.033730</td>\n",
       "      <td>-0.018366</td>\n",
       "      <td>-0.050737</td>\n",
       "      <td>-0.081061</td>\n",
       "      <td>-0.049644</td>\n",
       "      <td>1.373994</td>\n",
       "      <td>0.022401</td>\n",
       "      <td>-0.158224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>65321</td>\n",
       "      <td>1.778701</td>\n",
       "      <td>0.928243</td>\n",
       "      <td>88.297053</td>\n",
       "      <td>0.017218</td>\n",
       "      <td>0.069651</td>\n",
       "      <td>19.230952</td>\n",
       "      <td>0.343073</td>\n",
       "      <td>0.177788</td>\n",
       "      <td>0.209339</td>\n",
       "      <td>...</td>\n",
       "      <td>1.546102</td>\n",
       "      <td>0.665192</td>\n",
       "      <td>0.067298</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>-0.154135</td>\n",
       "      <td>-0.339544</td>\n",
       "      <td>-0.234140</td>\n",
       "      <td>-0.715521</td>\n",
       "      <td>0.796825</td>\n",
       "      <td>0.217828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>65322</td>\n",
       "      <td>1.727234</td>\n",
       "      <td>0.953996</td>\n",
       "      <td>200.042195</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.025753</td>\n",
       "      <td>111.745143</td>\n",
       "      <td>1.753307</td>\n",
       "      <td>0.794156</td>\n",
       "      <td>0.383115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820698</td>\n",
       "      <td>1.587214</td>\n",
       "      <td>1.410234</td>\n",
       "      <td>0.616368</td>\n",
       "      <td>0.173776</td>\n",
       "      <td>0.035674</td>\n",
       "      <td>-0.730042</td>\n",
       "      <td>-1.098227</td>\n",
       "      <td>-0.725404</td>\n",
       "      <td>0.922022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>65323</td>\n",
       "      <td>1.659698</td>\n",
       "      <td>0.929889</td>\n",
       "      <td>199.795106</td>\n",
       "      <td>-0.067536</td>\n",
       "      <td>-0.024107</td>\n",
       "      <td>-0.247089</td>\n",
       "      <td>2.333072</td>\n",
       "      <td>0.576625</td>\n",
       "      <td>0.365827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.907890</td>\n",
       "      <td>1.201815</td>\n",
       "      <td>0.579764</td>\n",
       "      <td>-0.217531</td>\n",
       "      <td>-0.017288</td>\n",
       "      <td>-0.001309</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.187860</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>-0.385398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>65324</td>\n",
       "      <td>1.591953</td>\n",
       "      <td>0.905493</td>\n",
       "      <td>199.799514</td>\n",
       "      <td>-0.067745</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>1.109957</td>\n",
       "      <td>1.053848</td>\n",
       "      <td>0.535494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954189</td>\n",
       "      <td>1.403103</td>\n",
       "      <td>-1.223115</td>\n",
       "      <td>0.477223</td>\n",
       "      <td>0.169667</td>\n",
       "      <td>0.075278</td>\n",
       "      <td>0.226886</td>\n",
       "      <td>0.262214</td>\n",
       "      <td>0.046299</td>\n",
       "      <td>0.201287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>65325</td>\n",
       "      <td>1.524215</td>\n",
       "      <td>0.881099</td>\n",
       "      <td>199.795273</td>\n",
       "      <td>-0.067738</td>\n",
       "      <td>-0.024394</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>1.655194</td>\n",
       "      <td>0.703917</td>\n",
       "      <td>0.586950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982748</td>\n",
       "      <td>1.331660</td>\n",
       "      <td>0.545237</td>\n",
       "      <td>-0.349931</td>\n",
       "      <td>0.051456</td>\n",
       "      <td>0.044785</td>\n",
       "      <td>-0.108138</td>\n",
       "      <td>-0.175571</td>\n",
       "      <td>0.028558</td>\n",
       "      <td>-0.071443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65326</th>\n",
       "      <td>65326</td>\n",
       "      <td>1.456470</td>\n",
       "      <td>0.856704</td>\n",
       "      <td>199.798635</td>\n",
       "      <td>-0.067744</td>\n",
       "      <td>-0.024394</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>1.448540</td>\n",
       "      <td>0.755009</td>\n",
       "      <td>0.589020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841524</td>\n",
       "      <td>1.240151</td>\n",
       "      <td>-0.206654</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.110683</td>\n",
       "      <td>0.285646</td>\n",
       "      <td>0.072288</td>\n",
       "      <td>-0.141223</td>\n",
       "      <td>-0.091509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65327</th>\n",
       "      <td>65327</td>\n",
       "      <td>1.388732</td>\n",
       "      <td>0.832309</td>\n",
       "      <td>199.796015</td>\n",
       "      <td>-0.067739</td>\n",
       "      <td>-0.024395</td>\n",
       "      <td>-0.002620</td>\n",
       "      <td>0.724342</td>\n",
       "      <td>0.777411</td>\n",
       "      <td>0.529293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901251</td>\n",
       "      <td>0.971965</td>\n",
       "      <td>-0.724198</td>\n",
       "      <td>0.022402</td>\n",
       "      <td>-0.059727</td>\n",
       "      <td>-0.196107</td>\n",
       "      <td>-0.158587</td>\n",
       "      <td>0.124964</td>\n",
       "      <td>0.059727</td>\n",
       "      <td>-0.268186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65328</th>\n",
       "      <td>65328</td>\n",
       "      <td>1.320988</td>\n",
       "      <td>0.807917</td>\n",
       "      <td>199.797343</td>\n",
       "      <td>-0.067743</td>\n",
       "      <td>-0.024393</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>1.720151</td>\n",
       "      <td>0.755582</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724617</td>\n",
       "      <td>1.229650</td>\n",
       "      <td>0.995809</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>0.051223</td>\n",
       "      <td>0.341809</td>\n",
       "      <td>-0.064732</td>\n",
       "      <td>0.545865</td>\n",
       "      <td>-0.176635</td>\n",
       "      <td>0.257685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65329</th>\n",
       "      <td>65329</td>\n",
       "      <td>1.253248</td>\n",
       "      <td>0.783521</td>\n",
       "      <td>199.797030</td>\n",
       "      <td>-0.067741</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>1.132271</td>\n",
       "      <td>0.605043</td>\n",
       "      <td>0.647688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557155</td>\n",
       "      <td>1.183981</td>\n",
       "      <td>-0.587880</td>\n",
       "      <td>-0.150539</td>\n",
       "      <td>0.067172</td>\n",
       "      <td>0.399509</td>\n",
       "      <td>0.302054</td>\n",
       "      <td>-0.133775</td>\n",
       "      <td>-0.167461</td>\n",
       "      <td>-0.045668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65330</th>\n",
       "      <td>65330</td>\n",
       "      <td>1.185506</td>\n",
       "      <td>0.759129</td>\n",
       "      <td>199.795937</td>\n",
       "      <td>-0.067741</td>\n",
       "      <td>-0.024392</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>1.253611</td>\n",
       "      <td>0.785374</td>\n",
       "      <td>0.902547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736063</td>\n",
       "      <td>1.292436</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>0.180331</td>\n",
       "      <td>0.254859</td>\n",
       "      <td>-0.434908</td>\n",
       "      <td>-0.182047</td>\n",
       "      <td>-0.131114</td>\n",
       "      <td>0.178908</td>\n",
       "      <td>0.108455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65331</th>\n",
       "      <td>65331</td>\n",
       "      <td>1.116709</td>\n",
       "      <td>0.740613</td>\n",
       "      <td>161.037295</td>\n",
       "      <td>-0.068797</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>-38.758642</td>\n",
       "      <td>1.403571</td>\n",
       "      <td>0.447402</td>\n",
       "      <td>1.214472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908414</td>\n",
       "      <td>1.214272</td>\n",
       "      <td>0.149960</td>\n",
       "      <td>-0.337972</td>\n",
       "      <td>0.311925</td>\n",
       "      <td>-0.059223</td>\n",
       "      <td>0.338359</td>\n",
       "      <td>-0.181220</td>\n",
       "      <td>0.172351</td>\n",
       "      <td>-0.078164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65332</th>\n",
       "      <td>65332</td>\n",
       "      <td>1.065723</td>\n",
       "      <td>0.789678</td>\n",
       "      <td>133.594735</td>\n",
       "      <td>-0.050986</td>\n",
       "      <td>0.049065</td>\n",
       "      <td>-27.442560</td>\n",
       "      <td>0.649794</td>\n",
       "      <td>0.793050</td>\n",
       "      <td>0.932239</td>\n",
       "      <td>...</td>\n",
       "      <td>1.380929</td>\n",
       "      <td>1.245964</td>\n",
       "      <td>-0.753777</td>\n",
       "      <td>0.345648</td>\n",
       "      <td>-0.282232</td>\n",
       "      <td>0.453133</td>\n",
       "      <td>-0.312272</td>\n",
       "      <td>-0.209846</td>\n",
       "      <td>0.472515</td>\n",
       "      <td>0.031692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65333</th>\n",
       "      <td>65333</td>\n",
       "      <td>1.016110</td>\n",
       "      <td>0.841931</td>\n",
       "      <td>125.831871</td>\n",
       "      <td>-0.049613</td>\n",
       "      <td>0.052253</td>\n",
       "      <td>-7.762864</td>\n",
       "      <td>0.503403</td>\n",
       "      <td>0.474554</td>\n",
       "      <td>1.026115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695036</td>\n",
       "      <td>1.080066</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.318496</td>\n",
       "      <td>0.093876</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.470148</td>\n",
       "      <td>0.284544</td>\n",
       "      <td>-0.685893</td>\n",
       "      <td>-0.165898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65334</th>\n",
       "      <td>65334</td>\n",
       "      <td>1.041929</td>\n",
       "      <td>0.897107</td>\n",
       "      <td>33.421213</td>\n",
       "      <td>0.025819</td>\n",
       "      <td>0.055176</td>\n",
       "      <td>-92.410658</td>\n",
       "      <td>0.947132</td>\n",
       "      <td>0.944721</td>\n",
       "      <td>0.967290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368698</td>\n",
       "      <td>0.650796</td>\n",
       "      <td>0.443728</td>\n",
       "      <td>0.470167</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>-0.030483</td>\n",
       "      <td>0.058087</td>\n",
       "      <td>-0.101546</td>\n",
       "      <td>-0.326338</td>\n",
       "      <td>-0.429270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65335</th>\n",
       "      <td>65335</td>\n",
       "      <td>1.101811</td>\n",
       "      <td>0.936815</td>\n",
       "      <td>33.585438</td>\n",
       "      <td>0.059882</td>\n",
       "      <td>0.039707</td>\n",
       "      <td>0.164225</td>\n",
       "      <td>0.694096</td>\n",
       "      <td>1.040242</td>\n",
       "      <td>0.933149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442840</td>\n",
       "      <td>0.622742</td>\n",
       "      <td>-0.253036</td>\n",
       "      <td>0.095521</td>\n",
       "      <td>-0.034141</td>\n",
       "      <td>0.054306</td>\n",
       "      <td>-0.270097</td>\n",
       "      <td>0.168264</td>\n",
       "      <td>0.074141</td>\n",
       "      <td>-0.028054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65336</th>\n",
       "      <td>65336</td>\n",
       "      <td>1.161784</td>\n",
       "      <td>0.976651</td>\n",
       "      <td>33.582240</td>\n",
       "      <td>0.059973</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>0.707737</td>\n",
       "      <td>0.704726</td>\n",
       "      <td>1.131393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402395</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>-0.335516</td>\n",
       "      <td>0.198244</td>\n",
       "      <td>-0.560561</td>\n",
       "      <td>-0.040804</td>\n",
       "      <td>-0.265593</td>\n",
       "      <td>-0.040444</td>\n",
       "      <td>-0.118342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65337</th>\n",
       "      <td>65337</td>\n",
       "      <td>1.221760</td>\n",
       "      <td>1.016492</td>\n",
       "      <td>33.586037</td>\n",
       "      <td>0.059976</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.823794</td>\n",
       "      <td>0.668811</td>\n",
       "      <td>0.952851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316505</td>\n",
       "      <td>0.535042</td>\n",
       "      <td>0.116057</td>\n",
       "      <td>-0.035915</td>\n",
       "      <td>-0.178542</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.447951</td>\n",
       "      <td>0.261467</td>\n",
       "      <td>-0.085890</td>\n",
       "      <td>0.030641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65338</th>\n",
       "      <td>65338</td>\n",
       "      <td>1.281731</td>\n",
       "      <td>1.056328</td>\n",
       "      <td>33.581797</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>0.674969</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299723</td>\n",
       "      <td>0.512812</td>\n",
       "      <td>-0.148824</td>\n",
       "      <td>0.188688</td>\n",
       "      <td>-0.029801</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.095421</td>\n",
       "      <td>-0.551787</td>\n",
       "      <td>-0.016782</td>\n",
       "      <td>-0.022229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65339</th>\n",
       "      <td>65339</td>\n",
       "      <td>1.341708</td>\n",
       "      <td>1.096169</td>\n",
       "      <td>33.586011</td>\n",
       "      <td>0.059977</td>\n",
       "      <td>0.039840</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.827843</td>\n",
       "      <td>0.693288</td>\n",
       "      <td>1.447567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342244</td>\n",
       "      <td>0.528701</td>\n",
       "      <td>0.152873</td>\n",
       "      <td>-0.164211</td>\n",
       "      <td>0.524516</td>\n",
       "      <td>0.360896</td>\n",
       "      <td>0.112749</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>0.042520</td>\n",
       "      <td>0.015889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65340</th>\n",
       "      <td>65340</td>\n",
       "      <td>1.401679</td>\n",
       "      <td>1.136005</td>\n",
       "      <td>33.581939</td>\n",
       "      <td>0.059970</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>0.716285</td>\n",
       "      <td>0.503528</td>\n",
       "      <td>0.960760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267632</td>\n",
       "      <td>0.447324</td>\n",
       "      <td>-0.111557</td>\n",
       "      <td>-0.189760</td>\n",
       "      <td>-0.486806</td>\n",
       "      <td>-0.333622</td>\n",
       "      <td>0.187956</td>\n",
       "      <td>0.024922</td>\n",
       "      <td>-0.074612</td>\n",
       "      <td>-0.081378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65341</th>\n",
       "      <td>65341</td>\n",
       "      <td>1.461656</td>\n",
       "      <td>1.175845</td>\n",
       "      <td>33.585412</td>\n",
       "      <td>0.059978</td>\n",
       "      <td>0.039840</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.654092</td>\n",
       "      <td>0.558887</td>\n",
       "      <td>1.290475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220357</td>\n",
       "      <td>0.392626</td>\n",
       "      <td>-0.062194</td>\n",
       "      <td>0.055359</td>\n",
       "      <td>0.329714</td>\n",
       "      <td>0.060022</td>\n",
       "      <td>-0.504622</td>\n",
       "      <td>-0.369053</td>\n",
       "      <td>-0.047274</td>\n",
       "      <td>-0.054698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65341 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         x         y       theta        dx        dy  \\\n",
       "1               1  0.850135  0.775909  168.212418 -0.070479  0.014711   \n",
       "2               2  0.779657  0.790625  168.209551 -0.070478  0.014716   \n",
       "3               3  0.709174  0.805340  168.212871 -0.070483  0.014715   \n",
       "4               4  0.638698  0.820056  168.208857 -0.070477  0.014716   \n",
       "5               5  0.568214  0.834773  168.212811 -0.070484  0.014717   \n",
       "6               6  0.499522  0.854553  134.137941 -0.068692  0.019781   \n",
       "7               7  0.471813  0.920420  111.074435 -0.027709  0.065866   \n",
       "8               8  0.445931  0.987610  111.074646 -0.025882  0.067190   \n",
       "9               9  0.418390  1.054107  114.933738 -0.027540  0.066498   \n",
       "10             10  0.394894  1.122021  107.231961 -0.023496  0.067914   \n",
       "11             11  0.373575  1.190786  107.229023 -0.021319  0.068765   \n",
       "12             12  0.353191  1.259852  103.356119 -0.020384  0.069066   \n",
       "13             13  0.321467  1.322560  154.129909 -0.031724  0.062708   \n",
       "14             14  0.254388  1.310835  205.548630 -0.067079 -0.011725   \n",
       "15             15  0.188872  1.283017  192.331127 -0.065516 -0.027818   \n",
       "16             16  0.144871  1.319933   78.532227 -0.044002  0.036916   \n",
       "17             17  0.199346  1.328425  295.983981  0.054475  0.008492   \n",
       "18             18  0.223847  1.261762  285.663108  0.024501 -0.066664   \n",
       "19             19  0.243284  1.192453  285.672356  0.019437 -0.069309   \n",
       "20             20  0.262726  1.123126  285.673771  0.019442 -0.069327   \n",
       "21             21  0.282172  1.053802  285.673408  0.019445 -0.069323   \n",
       "22             22  0.301613  0.984477  285.672277  0.019441 -0.069325   \n",
       "23             23  0.317117  0.914189  281.826766  0.015504 -0.070288   \n",
       "24             24  0.331865  0.843719  281.823130  0.014747 -0.070471   \n",
       "25             25  0.346615  0.773243  281.827439  0.014750 -0.070476   \n",
       "26             26  0.361362  0.702773  281.822447  0.014748 -0.070470   \n",
       "27             27  0.376111  0.632296  281.827300  0.014749 -0.070477   \n",
       "28             28  0.390860  0.561827  281.822443  0.014748 -0.070469   \n",
       "29             29  0.405607  0.491350  281.826488  0.014748 -0.070477   \n",
       "30             30  0.365783  0.451031  166.530722 -0.039825 -0.040319   \n",
       "...           ...       ...       ...         ...       ...       ...   \n",
       "65312       65312  1.506068  0.983709  286.798226 -0.040986 -0.043939   \n",
       "65313       65313  1.525680  0.914969  286.169047  0.019612 -0.068740   \n",
       "65314       65314  1.545727  0.845812  286.173240  0.020047 -0.069157   \n",
       "65315       65315  1.565770  0.776662  286.168788  0.020044 -0.069150   \n",
       "65316       65316  1.585816  0.707505  286.172872  0.020045 -0.069157   \n",
       "65317       65317  1.637710  0.683292   46.306106  0.051895 -0.024212   \n",
       "65318       65318  1.681473  0.739371   53.595364  0.043763  0.056078   \n",
       "65319       65319  1.724195  0.797321   53.591446  0.042722  0.057950   \n",
       "65320       65320  1.761483  0.858592   69.066101  0.037287  0.061271   \n",
       "65321       65321  1.778701  0.928243   88.297053  0.017218  0.069651   \n",
       "65322       65322  1.727234  0.953996  200.042195 -0.051467  0.025753   \n",
       "65323       65323  1.659698  0.929889  199.795106 -0.067536 -0.024107   \n",
       "65324       65324  1.591953  0.905493  199.799514 -0.067745 -0.024396   \n",
       "65325       65325  1.524215  0.881099  199.795273 -0.067738 -0.024394   \n",
       "65326       65326  1.456470  0.856704  199.798635 -0.067744 -0.024394   \n",
       "65327       65327  1.388732  0.832309  199.796015 -0.067739 -0.024395   \n",
       "65328       65328  1.320988  0.807917  199.797343 -0.067743 -0.024393   \n",
       "65329       65329  1.253248  0.783521  199.797030 -0.067741 -0.024396   \n",
       "65330       65330  1.185506  0.759129  199.795937 -0.067741 -0.024392   \n",
       "65331       65331  1.116709  0.740613  161.037295 -0.068797 -0.018516   \n",
       "65332       65332  1.065723  0.789678  133.594735 -0.050986  0.049065   \n",
       "65333       65333  1.016110  0.841931  125.831871 -0.049613  0.052253   \n",
       "65334       65334  1.041929  0.897107   33.421213  0.025819  0.055176   \n",
       "65335       65335  1.101811  0.936815   33.585438  0.059882  0.039707   \n",
       "65336       65336  1.161784  0.976651   33.582240  0.059973  0.039836   \n",
       "65337       65337  1.221760  1.016492   33.586037  0.059976  0.039841   \n",
       "65338       65338  1.281731  1.056328   33.581797  0.059971  0.039836   \n",
       "65339       65339  1.341708  1.096169   33.586011  0.059977  0.039840   \n",
       "65340       65340  1.401679  1.136005   33.581939  0.059970  0.039837   \n",
       "65341       65341  1.461656  1.175845   33.585412  0.059978  0.039840   \n",
       "\n",
       "           dtheta  sensor_1  sensor_2  sensor_3    ...      sensor_7  \\\n",
       "1        0.002935  0.571635  0.596799  0.883340    ...      0.830057   \n",
       "2       -0.002867  0.581452  0.904627  0.689004    ...      0.491200   \n",
       "3        0.003319  0.956302  0.842911  0.796714    ...      1.246415   \n",
       "4       -0.004013  0.671731  0.779896  0.962191    ...      0.567806   \n",
       "5        0.003954  0.751139  0.495261  0.742213    ...      0.575172   \n",
       "6      -34.074870  0.736352  0.588845  1.498086    ...      0.485124   \n",
       "7      -23.063507  0.419531  1.069214  1.768725    ...      0.310053   \n",
       "8        0.000212  0.536241  0.483614  1.371695    ...      0.405266   \n",
       "9        3.859092  0.342284  0.720183  1.895051    ...      0.428127   \n",
       "10      -7.701777  0.244309  0.488843  1.385274    ...      0.421095   \n",
       "11      -0.002939  0.295913  0.457004  1.759191    ...      0.271917   \n",
       "12      -3.872904  0.133681  0.255816  2.019363    ...      0.374215   \n",
       "13      50.773790  0.198410  0.107779  0.223307    ...      0.429172   \n",
       "14      51.418721  0.154634  0.271346  0.138330    ...      1.173755   \n",
       "15     -13.217502  0.169631  0.233973  0.175622    ...      1.432796   \n",
       "16    -113.798900  0.197391  0.833326  2.133678    ...      0.110615   \n",
       "17     217.451754  1.446887  0.192367  0.161671    ...      2.085222   \n",
       "18     -10.320873  1.057248  0.245830  0.154252    ...      1.996760   \n",
       "19       0.009248  1.093535  0.292819  0.226315    ...      1.907153   \n",
       "20       0.001415  1.122716  0.326370  0.281348    ...      2.113977   \n",
       "21      -0.000363  1.106262  0.268065  0.208852    ...      1.793466   \n",
       "22      -0.001131  0.700907  0.415374  0.257344    ...      1.617733   \n",
       "23      -3.845511  1.053929  0.329018  0.295675    ...      1.047159   \n",
       "24      -0.003636  0.689328  0.333630  0.177559    ...      1.867602   \n",
       "25       0.004310  0.729204  0.450479  0.301668    ...      1.143278   \n",
       "26      -0.004992  0.740336  0.423206  0.385625    ...      1.824108   \n",
       "27       0.004853  0.513066  0.419759  0.362323    ...      1.943009   \n",
       "28      -0.004858  0.280831  0.524966  0.453390    ...      1.671884   \n",
       "29       0.004046  0.379643  0.529145  0.447030    ...      1.305905   \n",
       "30    -115.295766  0.252419  1.087424  1.342140    ...      0.549343   \n",
       "...           ...       ...       ...       ...    ...           ...   \n",
       "65312   96.847750  0.889909  1.174579  1.463482    ...      0.463487   \n",
       "65313   -0.629179  0.866091  1.797614  1.762390    ...      0.461549   \n",
       "65314    0.004193  0.685343  1.273496  1.331721    ...      0.363549   \n",
       "65315   -0.004452  0.811429  1.996541  1.471416    ...      0.427246   \n",
       "65316    0.004085  0.761846  0.875321  1.677064    ...      0.451773   \n",
       "65317 -239.866767  0.387084  0.370887  0.530013    ...      1.003578   \n",
       "65318    7.289258  0.283493  0.257740  0.481433    ...      0.937412   \n",
       "65319   -0.003919  0.242045  0.241740  0.414211    ...      0.726876   \n",
       "65320   15.474655  0.275775  0.223374  0.363474    ...      0.749277   \n",
       "65321   19.230952  0.343073  0.177788  0.209339    ...      1.546102   \n",
       "65322  111.745143  1.753307  0.794156  0.383115    ...      0.820698   \n",
       "65323   -0.247089  2.333072  0.576625  0.365827    ...      0.907890   \n",
       "65324    0.004408  1.109957  1.053848  0.535494    ...      0.954189   \n",
       "65325   -0.004241  1.655194  0.703917  0.586950    ...      0.982748   \n",
       "65326    0.003362  1.448540  0.755009  0.589020    ...      0.841524   \n",
       "65327   -0.002620  0.724342  0.777411  0.529293    ...      0.901251   \n",
       "65328    0.001328  1.720151  0.755582  0.580516    ...      0.724617   \n",
       "65329   -0.000313  1.132271  0.605043  0.647688    ...      0.557155   \n",
       "65330   -0.001094  1.253611  0.785374  0.902547    ...      0.736063   \n",
       "65331  -38.758642  1.403571  0.447402  1.214472    ...      0.908414   \n",
       "65332  -27.442560  0.649794  0.793050  0.932239    ...      1.380929   \n",
       "65333   -7.762864  0.503403  0.474554  1.026115    ...      0.695036   \n",
       "65334  -92.410658  0.947132  0.944721  0.967290    ...      0.368698   \n",
       "65335    0.164225  0.694096  1.040242  0.933149    ...      0.442840   \n",
       "65336   -0.003199  0.707737  0.704726  1.131393    ...      0.402395   \n",
       "65337    0.003798  0.823794  0.668811  0.952851    ...      0.316505   \n",
       "65338   -0.004241  0.674969  0.857500  0.923050    ...      0.299723   \n",
       "65339    0.004214  0.827843  0.693288  1.447567    ...      0.342244   \n",
       "65340   -0.004072  0.716285  0.503528  0.960760    ...      0.267632   \n",
       "65341    0.003473  0.654092  0.558887  1.290475    ...      0.220357   \n",
       "\n",
       "       sensor_8  dsensor_1  dsensor_2  dsensor_3  dsensor_4  dsensor_5  \\\n",
       "1      1.028332  -0.513544  -0.193468  -0.010002  -0.430864  -0.070277   \n",
       "2      0.889130   0.009817   0.307828  -0.194336   0.239518   0.206480   \n",
       "3      0.712158   0.374849  -0.061716   0.107710   0.075412  -0.345782   \n",
       "4      0.595164  -0.284570  -0.063014   0.165477   0.005216   0.128150   \n",
       "5      0.523609   0.079408  -0.284635  -0.219978  -0.447768   0.549075   \n",
       "6      0.411577  -0.014787   0.093583   0.755874   0.881613  -0.946182   \n",
       "7      0.803526  -0.316821   0.480369   0.270639  -0.734570   0.072953   \n",
       "8      0.604981   0.116710  -0.585600  -0.397030  -0.102371   0.026730   \n",
       "9      0.524840  -0.193957   0.236569   0.523355   1.116081   0.363247   \n",
       "10     0.502860  -0.097975  -0.231340  -0.509776  -0.739754  -0.281965   \n",
       "11     0.375203   0.051604  -0.031839   0.373917  -0.190467   0.580860   \n",
       "12     0.264384  -0.162232  -0.201188   0.260172   0.862895  -0.851819   \n",
       "13     0.260524   0.064729  -0.148038  -1.796056  -0.521964   0.646311   \n",
       "14     0.389340  -0.043776   0.163567  -0.084977  -1.270904   0.027127   \n",
       "15     0.197966   0.014998  -0.037373   0.037292   0.108587   0.541179   \n",
       "16     0.080214   0.027760   0.599354   1.958056   1.054939  -1.634324   \n",
       "17     0.942276   1.249496  -0.640960  -1.972006  -1.135311  -0.091708   \n",
       "18     1.442333  -0.389640   0.053463  -0.007419   0.087801   0.063392   \n",
       "19     1.584347   0.036288   0.046989   0.072063   0.037655   0.078275   \n",
       "20     1.741571   0.029180   0.033551   0.055033   0.110055   0.005581   \n",
       "21     1.437905  -0.016454  -0.058305  -0.072496   0.035736   0.013442   \n",
       "22     1.353854  -0.405354   0.147309   0.048493   0.047170   0.182783   \n",
       "23     0.601494   0.353021  -0.086356   0.038331  -0.022708  -0.026225   \n",
       "24     1.049892  -0.364600   0.004611  -0.118116  -0.010178   0.422480   \n",
       "25     0.952727   0.039875   0.116849   0.124109   0.204662  -0.259174   \n",
       "26     0.970899   0.011132  -0.027273   0.083958   0.077142   0.515720   \n",
       "27     0.735944  -0.227270  -0.003446  -0.023302  -0.235438  -0.250889   \n",
       "28     0.657895  -0.232236   0.105206   0.091066   0.157780   0.052931   \n",
       "29     0.451267   0.098813   0.004179  -0.006359   0.035503  -0.002099   \n",
       "30     0.318731  -0.127225   0.558279   0.895110   0.680145  -0.279006   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "65312  0.895936  -0.397189   0.402950   0.940960  -0.017752   0.188211   \n",
       "65313  0.551110  -0.023818   0.623036   0.298908   0.476274  -0.085996   \n",
       "65314  0.698678  -0.180747  -0.524118  -0.430669   0.001613   0.197342   \n",
       "65315  0.484253   0.126085   0.723045   0.139695   0.123192  -0.125717   \n",
       "65316  0.705448  -0.049582  -1.121219   0.205648   0.394226   0.497829   \n",
       "65317  1.061082  -0.374762  -0.504434  -1.147051  -0.771560   0.591829   \n",
       "65318  0.780959  -0.103591  -0.113147  -0.048580   0.065553  -0.499304   \n",
       "65319  0.605587  -0.041448  -0.016000  -0.067222   0.048685   0.056658   \n",
       "65320  0.447364   0.033730  -0.018366  -0.050737  -0.081061  -0.049644   \n",
       "65321  0.665192   0.067298  -0.045585  -0.154135  -0.339544  -0.234140   \n",
       "65322  1.587214   1.410234   0.616368   0.173776   0.035674  -0.730042   \n",
       "65323  1.201815   0.579764  -0.217531  -0.017288  -0.001309   0.073834   \n",
       "65324  1.403103  -1.223115   0.477223   0.169667   0.075278   0.226886   \n",
       "65325  1.331660   0.545237  -0.349931   0.051456   0.044785  -0.108138   \n",
       "65326  1.240151  -0.206654   0.051092   0.002070   0.110683   0.285646   \n",
       "65327  0.971965  -0.724198   0.022402  -0.059727  -0.196107  -0.158587   \n",
       "65328  1.229650   0.995809  -0.021828   0.051223   0.341809  -0.064732   \n",
       "65329  1.183981  -0.587880  -0.150539   0.067172   0.399509   0.302054   \n",
       "65330  1.292436   0.121340   0.180331   0.254859  -0.434908  -0.182047   \n",
       "65331  1.214272   0.149960  -0.337972   0.311925  -0.059223   0.338359   \n",
       "65332  1.245964  -0.753777   0.345648  -0.282232   0.453133  -0.312272   \n",
       "65333  1.080066  -0.146391  -0.318496   0.093876   0.000168   0.470148   \n",
       "65334  0.650796   0.443728   0.470167  -0.058825  -0.030483   0.058087   \n",
       "65335  0.622742  -0.253036   0.095521  -0.034141   0.054306  -0.270097   \n",
       "65336  0.504400   0.013641  -0.335516   0.198244  -0.560561  -0.040804   \n",
       "65337  0.535042   0.116057  -0.035915  -0.178542   0.074935   0.447951   \n",
       "65338  0.512812  -0.148824   0.188688  -0.029801   0.588889   0.095421   \n",
       "65339  0.528701   0.152873  -0.164211   0.524516   0.360896   0.112749   \n",
       "65340  0.447324  -0.111557  -0.189760  -0.486806  -0.333622   0.187956   \n",
       "65341  0.392626  -0.062194   0.055359   0.329714   0.060022  -0.504622   \n",
       "\n",
       "       dsensor_6  dsensor_7  dsensor_8  \n",
       "1      -0.387726  -0.309733  -0.116568  \n",
       "2       0.293382  -0.338857  -0.139203  \n",
       "3      -0.084918   0.755215  -0.176971  \n",
       "4      -0.054777  -0.678608  -0.116994  \n",
       "5       0.118147   0.007365  -0.071555  \n",
       "6       0.247688  -0.090048  -0.112032  \n",
       "7      -0.706186  -0.175071   0.391949  \n",
       "8      -0.056112   0.095213  -0.198545  \n",
       "9      -0.000967   0.022861  -0.080141  \n",
       "10     -0.093100  -0.007032  -0.021980  \n",
       "11      0.144419  -0.149179  -0.127657  \n",
       "12     -0.154971   0.102298  -0.110818  \n",
       "13      1.244288   0.054956  -0.003861  \n",
       "14     -0.022878   0.744584   0.128816  \n",
       "15      0.215615   0.259041  -0.191374  \n",
       "16     -1.665918  -1.322181  -0.117752  \n",
       "17      0.088676   1.974607   0.862061  \n",
       "18      0.129924  -0.088462   0.500058  \n",
       "19      0.197717  -0.089606   0.142014  \n",
       "20      0.151624   0.206824   0.157224  \n",
       "21      0.174463  -0.320511  -0.303666  \n",
       "22      0.116906  -0.175733  -0.084051  \n",
       "23     -0.137646  -0.570574  -0.752361  \n",
       "24      0.247598   0.820444   0.448398  \n",
       "25     -0.262489  -0.724324  -0.097165  \n",
       "26      0.722845   0.680829   0.018172  \n",
       "27      0.312916   0.118902  -0.234954  \n",
       "28     -0.270492  -0.271126  -0.078049  \n",
       "29      0.203375  -0.365979  -0.206628  \n",
       "30     -1.400963  -0.756562  -0.132536  \n",
       "...          ...        ...        ...  \n",
       "65312  -0.035631  -0.353121  -1.419340  \n",
       "65313  -0.267685  -0.001938  -0.344826  \n",
       "65314   0.101272  -0.098000   0.147568  \n",
       "65315  -0.027803   0.063697  -0.214426  \n",
       "65316   0.070358   0.024527   0.221195  \n",
       "65317   1.059905   0.551805   0.355633  \n",
       "65318  -0.140265  -0.066165  -0.280123  \n",
       "65319  -0.760184  -0.210536  -0.175372  \n",
       "65320   1.373994   0.022401  -0.158224  \n",
       "65321  -0.715521   0.796825   0.217828  \n",
       "65322  -1.098227  -0.725404   0.922022  \n",
       "65323   0.187860   0.087192  -0.385398  \n",
       "65324   0.262214   0.046299   0.201287  \n",
       "65325  -0.175571   0.028558  -0.071443  \n",
       "65326   0.072288  -0.141223  -0.091509  \n",
       "65327   0.124964   0.059727  -0.268186  \n",
       "65328   0.545865  -0.176635   0.257685  \n",
       "65329  -0.133775  -0.167461  -0.045668  \n",
       "65330  -0.131114   0.178908   0.108455  \n",
       "65331  -0.181220   0.172351  -0.078164  \n",
       "65332  -0.209846   0.472515   0.031692  \n",
       "65333   0.284544  -0.685893  -0.165898  \n",
       "65334  -0.101546  -0.326338  -0.429270  \n",
       "65335   0.168264   0.074141  -0.028054  \n",
       "65336  -0.265593  -0.040444  -0.118342  \n",
       "65337   0.261467  -0.085890   0.030641  \n",
       "65338  -0.551787  -0.016782  -0.022229  \n",
       "65339   0.277871   0.042520   0.015889  \n",
       "65340   0.024922  -0.074612  -0.081378  \n",
       "65341  -0.369053  -0.047274  -0.054698  \n",
       "\n",
       "[65341 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>0.504753</td>\n",
       "      <td>0.502063</td>\n",
       "      <td>0.499785</td>\n",
       "      <td>0.500412</td>\n",
       "      <td>0.501624</td>\n",
       "      <td>0.239976</td>\n",
       "      <td>0.236145</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251293</td>\n",
       "      <td>0.242889</td>\n",
       "      <td>0.449485</td>\n",
       "      <td>0.468761</td>\n",
       "      <td>0.513828</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>0.519272</td>\n",
       "      <td>0.531825</td>\n",
       "      <td>0.446832</td>\n",
       "      <td>0.426383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288682</td>\n",
       "      <td>0.272549</td>\n",
       "      <td>0.264025</td>\n",
       "      <td>0.290735</td>\n",
       "      <td>0.353425</td>\n",
       "      <td>0.335002</td>\n",
       "      <td>0.114192</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.149030</td>\n",
       "      <td>0.169722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160999</td>\n",
       "      <td>0.143636</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.077416</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.081846</td>\n",
       "      <td>0.080184</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255777</td>\n",
       "      <td>0.269104</td>\n",
       "      <td>0.251332</td>\n",
       "      <td>0.139102</td>\n",
       "      <td>0.181098</td>\n",
       "      <td>0.496242</td>\n",
       "      <td>0.127116</td>\n",
       "      <td>0.108774</td>\n",
       "      <td>0.119219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112714</td>\n",
       "      <td>0.127502</td>\n",
       "      <td>0.412053</td>\n",
       "      <td>0.436092</td>\n",
       "      <td>0.483323</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>0.488782</td>\n",
       "      <td>0.501383</td>\n",
       "      <td>0.415117</td>\n",
       "      <td>0.388997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498537</td>\n",
       "      <td>0.503544</td>\n",
       "      <td>0.498631</td>\n",
       "      <td>0.500020</td>\n",
       "      <td>0.500797</td>\n",
       "      <td>0.501627</td>\n",
       "      <td>0.216759</td>\n",
       "      <td>0.215659</td>\n",
       "      <td>0.237666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224880</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.442927</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>0.516314</td>\n",
       "      <td>0.512401</td>\n",
       "      <td>0.524693</td>\n",
       "      <td>0.534465</td>\n",
       "      <td>0.445713</td>\n",
       "      <td>0.419784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735371</td>\n",
       "      <td>0.740156</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.860369</td>\n",
       "      <td>0.821281</td>\n",
       "      <td>0.506991</td>\n",
       "      <td>0.328127</td>\n",
       "      <td>0.337889</td>\n",
       "      <td>0.375692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360337</td>\n",
       "      <td>0.333973</td>\n",
       "      <td>0.479438</td>\n",
       "      <td>0.498031</td>\n",
       "      <td>0.547039</td>\n",
       "      <td>0.544322</td>\n",
       "      <td>0.557577</td>\n",
       "      <td>0.565527</td>\n",
       "      <td>0.475271</td>\n",
       "      <td>0.456114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             x             y         theta            dx  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.500000      0.498321      0.504753      0.502063      0.499785   \n",
       "std        0.288682      0.272549      0.264025      0.290735      0.353425   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.255777      0.269104      0.251332      0.139102   \n",
       "50%        0.500000      0.498537      0.503544      0.498631      0.500020   \n",
       "75%        0.750000      0.735371      0.740156      0.752400      0.860369   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 dy        dtheta      sensor_1      sensor_2      sensor_3  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.500412      0.501624      0.239976      0.236145      0.261438   \n",
       "std        0.335002      0.114192      0.140647      0.149030      0.169722   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.181098      0.496242      0.127116      0.108774      0.119219   \n",
       "50%        0.500797      0.501627      0.216759      0.215659      0.237666   \n",
       "75%        0.821281      0.506991      0.328127      0.337889      0.375692   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...           sensor_7      sensor_8     dsensor_1     dsensor_2  \\\n",
       "count      ...       65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       ...           0.251293      0.242889      0.449485      0.468761   \n",
       "std        ...           0.160999      0.143636      0.078247      0.073403   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.112714      0.127502      0.412053      0.436092   \n",
       "50%        ...           0.224880      0.220298      0.442927      0.467409   \n",
       "75%        ...           0.360337      0.333973      0.479438      0.498031   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          dsensor_3     dsensor_4     dsensor_5     dsensor_6     dsensor_7  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.513828      0.507022      0.519272      0.531825      0.446832   \n",
       "std        0.077416      0.078125      0.081846      0.080184      0.072415   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.483323      0.477268      0.488782      0.501383      0.415117   \n",
       "50%        0.516314      0.512401      0.524693      0.534465      0.445713   \n",
       "75%        0.547039      0.544322      0.557577      0.565527      0.475271   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          dsensor_8  \n",
       "count  65341.000000  \n",
       "mean       0.426383  \n",
       "std        0.077850  \n",
       "min        0.000000  \n",
       "25%        0.388997  \n",
       "50%        0.419784  \n",
       "75%        0.456114  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be split into training, testing and validation sets. 60% of the data will be used for training, 20% for training and 20% of validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train size\n",
    "test_size_percentage = .2\n",
    "train_size_percentage = .6\n",
    "ds_size = normalized_df.shape[0]\n",
    "train_size = int(train_size_percentage * ds_size)\n",
    "test_size = int(test_size_percentage * ds_size)\n",
    "\n",
    "# shuffle dataset\n",
    "normalized_df = normalized_df.sample(frac=1)\n",
    "\n",
    "# separate inputs from outputs\n",
    "inputs = normalized_df[['x', 'y', 'theta']]\n",
    "targets = normalized_df[['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8']]\n",
    "\n",
    "# train\n",
    "train_inputs = inputs[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "\n",
    "# test\n",
    "test_inputs = inputs[train_size:(train_size + test_size)]\n",
    "test_targets = targets[train_size:(train_size + test_size)]\n",
    "\n",
    "# validation\n",
    "validation_inputs = inputs[(train_size + test_size):]\n",
    "validation_targets = targets[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input the neural network receives the x, y coordinates and rotation angle $\\theta$. The output are the sensor measurements. One model per sensor will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    # neural network with a 10-neuron hidden layer\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(10, activation='relu', input_shape=(3,)))\n",
    "#     model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(6, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "#     rmsprop = optimizers.RMSprop(learning_rate=0.01)\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets[['sensor_1']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(sensor_number, num_epochs=10, k=5):\n",
    "    num_val_samples = len(train_inputs) // k\n",
    "    validation_scores = []\n",
    "    histories = []\n",
    "    nmse = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        val_data = train_inputs[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[[sensor_number]][i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_inputs[:i * num_val_samples],\n",
    "             train_inputs[(i + 1) * num_val_samples:]], axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [train_targets[[sensor_number]][:i * num_val_samples],\n",
    "             train_targets[[sensor_number]][(i + 1) * num_val_samples:]], axis=0)\n",
    "\n",
    "\n",
    "        model = get_model()\n",
    "\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                            validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=1, verbose=1)\n",
    "        histories.append(history.history)\n",
    "\n",
    "        predictions_targets = model.predict(val_data)\n",
    "        nmse.append(np.mean((predictions_targets - val_targets)**2)/np.var(val_targets))\n",
    "        \n",
    "    return histories, nmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 31364 samples, validate on 7840 samples\n",
      "Epoch 1/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0116 - mae: 0.0814 - val_loss: 0.0067 - val_mae: 0.0624\n",
      "Epoch 2/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0069 - mae: 0.0615 - val_loss: 0.0058 - val_mae: 0.0576\n",
      "Epoch 3/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0065 - mae: 0.0602 - val_loss: 0.0058 - val_mae: 0.0556\n",
      "Epoch 4/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0064 - mae: 0.0598 - val_loss: 0.0076 - val_mae: 0.0702\n",
      "Epoch 5/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0064 - mae: 0.0594 - val_loss: 0.0059 - val_mae: 0.0583\n",
      "Epoch 6/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0064 - mae: 0.0593 - val_loss: 0.0068 - val_mae: 0.0641\n",
      "Epoch 7/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0060 - mae: 0.0578 - val_loss: 0.0054 - val_mae: 0.0524\n",
      "Epoch 8/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0059 - mae: 0.0571 - val_loss: 0.0050 - val_mae: 0.0516\n",
      "Epoch 9/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0057 - mae: 0.0561 - val_loss: 0.0057 - val_mae: 0.0560\n",
      "Epoch 10/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0056 - mae: 0.0555 - val_loss: 0.0053 - val_mae: 0.0538\n",
      "Epoch 11/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0056 - mae: 0.0554 - val_loss: 0.0053 - val_mae: 0.0526\n",
      "Epoch 12/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0055 - mae: 0.0552 - val_loss: 0.0051 - val_mae: 0.0512\n",
      "Epoch 13/150\n",
      "31364/31364 [==============================] - 41s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0048 - val_mae: 0.0503\n",
      "Epoch 14/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0048 - val_mae: 0.0512\n",
      "Epoch 15/150\n",
      "31364/31364 [==============================] - 39s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0052 - val_mae: 0.0515\n",
      "Epoch 16/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0046 - val_mae: 0.0504\n",
      "Epoch 17/150\n",
      "31364/31364 [==============================] - 39s 1ms/step - loss: 0.0052 - mae: 0.0534 - val_loss: 0.0048 - val_mae: 0.0501\n",
      "Epoch 18/150\n",
      "31364/31364 [==============================] - 40s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0052 - val_mae: 0.0527\n",
      "Epoch 19/150\n",
      "31364/31364 [==============================] - 40s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0048 - val_mae: 0.0528\n",
      "Epoch 20/150\n",
      "31364/31364 [==============================] - 40s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0059 - val_mae: 0.0561\n",
      "Epoch 21/150\n",
      "31364/31364 [==============================] - 42s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0045 - val_mae: 0.0491\n",
      "Epoch 22/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0052 - mae: 0.0533 - val_loss: 0.0046 - val_mae: 0.0508\n",
      "Epoch 23/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0049 - val_mae: 0.0536\n",
      "Epoch 24/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0052 - mae: 0.0533 - val_loss: 0.0047 - val_mae: 0.0522\n",
      "Epoch 25/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0045 - val_mae: 0.0491\n",
      "Epoch 26/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0045 - val_mae: 0.0504\n",
      "Epoch 27/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0045 - val_mae: 0.0496\n",
      "Epoch 28/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0082 - val_mae: 0.0729\n",
      "Epoch 29/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0049 - mae: 0.0520 - val_loss: 0.0047 - val_mae: 0.0508\n",
      "Epoch 30/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0070 - val_mae: 0.0644\n",
      "Epoch 31/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0044 - val_mae: 0.0497\n",
      "Epoch 32/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0049 - val_mae: 0.0525\n",
      "Epoch 33/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0049 - mae: 0.0522 - val_loss: 0.0054 - val_mae: 0.0566\n",
      "Epoch 34/150\n",
      "31364/31364 [==============================] - 38s 1ms/step - loss: 0.0049 - mae: 0.0522 - val_loss: 0.0050 - val_mae: 0.0526\n",
      "Epoch 35/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0523 - val_loss: 0.0042 - val_mae: 0.0478\n",
      "Epoch 36/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0522 - val_loss: 0.0072 - val_mae: 0.0679\n",
      "Epoch 37/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0042 - val_mae: 0.0481\n",
      "Epoch 38/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.0042 - val_mae: 0.0478\n",
      "Epoch 39/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0052 - val_mae: 0.0566\n",
      "Epoch 40/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0045 - val_mae: 0.0506\n",
      "Epoch 41/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0050 - val_mae: 0.0530\n",
      "Epoch 42/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0043 - val_mae: 0.0495\n",
      "Epoch 43/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0519 - val_loss: 0.0043 - val_mae: 0.0481\n",
      "Epoch 44/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.0056 - val_mae: 0.0587\n",
      "Epoch 45/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0045 - val_mae: 0.0499\n",
      "Epoch 46/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0041 - val_mae: 0.0461\n",
      "Epoch 47/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0065 - val_mae: 0.0616\n",
      "Epoch 48/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0048 - val_mae: 0.0515\n",
      "Epoch 49/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0043 - val_mae: 0.0494\n",
      "Epoch 50/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.0048 - val_mae: 0.0517\n",
      "Epoch 51/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.0047 - val_mae: 0.0508\n",
      "Epoch 52/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0514 - val_loss: 0.0044 - val_mae: 0.0486\n",
      "Epoch 53/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0056 - val_mae: 0.0583\n",
      "Epoch 54/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0047 - val_mae: 0.0514\n",
      "Epoch 55/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0508 - val_loss: 0.0041 - val_mae: 0.0466\n",
      "Epoch 56/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0510 - val_loss: 0.0049 - val_mae: 0.0531\n",
      "Epoch 57/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0510 - val_loss: 0.0045 - val_mae: 0.0509\n",
      "Epoch 58/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0508 - val_loss: 0.0043 - val_mae: 0.0488\n",
      "Epoch 59/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0047 - mae: 0.0510 - val_loss: 0.0058 - val_mae: 0.0607\n",
      "Epoch 60/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0042 - val_mae: 0.0480\n",
      "Epoch 61/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0048 - val_mae: 0.0527\n",
      "Epoch 62/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0042 - val_mae: 0.0473\n",
      "Epoch 63/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0040 - val_mae: 0.0462\n",
      "Epoch 64/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0054 - val_mae: 0.0556\n",
      "Epoch 65/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.0056 - val_mae: 0.0567\n",
      "Epoch 66/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0044 - val_mae: 0.0493\n",
      "Epoch 67/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0045 - val_mae: 0.0491\n",
      "Epoch 68/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0045 - val_mae: 0.0497\n",
      "Epoch 69/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0042 - val_mae: 0.0474\n",
      "Epoch 70/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0045 - val_mae: 0.0491\n",
      "Epoch 71/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0043 - val_mae: 0.0483\n",
      "Epoch 72/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0065 - val_mae: 0.0639\n",
      "Epoch 73/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0508 - val_loss: 0.0042 - val_mae: 0.0477\n",
      "Epoch 74/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0507 - val_loss: 0.0050 - val_mae: 0.0544\n",
      "Epoch 75/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0506 - val_loss: 0.0047 - val_mae: 0.0500\n",
      "Epoch 76/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0507 - val_loss: 0.0044 - val_mae: 0.0485\n",
      "Epoch 77/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0503 - val_loss: 0.0044 - val_mae: 0.0500\n",
      "Epoch 78/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0046 - mae: 0.0502 - val_loss: 0.0053 - val_mae: 0.0568\n",
      "Epoch 79/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0046 - mae: 0.0502 - val_loss: 0.0046 - val_mae: 0.0506\n",
      "Epoch 80/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0046 - mae: 0.0501 - val_loss: 0.0046 - val_mae: 0.0493\n",
      "Epoch 81/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0046 - mae: 0.0502 - val_loss: 0.0043 - val_mae: 0.0472\n",
      "Epoch 82/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0046 - mae: 0.0501 - val_loss: 0.0044 - val_mae: 0.0490\n",
      "Epoch 83/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0046 - mae: 0.0503 - val_loss: 0.0054 - val_mae: 0.0554\n",
      "Epoch 84/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0504 - val_loss: 0.0046 - val_mae: 0.0504\n",
      "Epoch 85/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0504 - val_loss: 0.0041 - val_mae: 0.0468\n",
      "Epoch 86/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0506 - val_loss: 0.0043 - val_mae: 0.0479\n",
      "Epoch 87/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0507 - val_loss: 0.0053 - val_mae: 0.0541\n",
      "Epoch 88/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0506 - val_loss: 0.0067 - val_mae: 0.0661\n",
      "Epoch 89/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0044 - val_mae: 0.0497\n",
      "Epoch 90/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0053 - val_mae: 0.0561\n",
      "Epoch 91/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0043 - val_mae: 0.0488\n",
      "Epoch 92/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0061 - val_mae: 0.0595\n",
      "Epoch 93/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0043 - val_mae: 0.0472\n",
      "Epoch 94/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0045 - val_mae: 0.0490\n",
      "Epoch 95/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0507 - val_loss: 0.0064 - val_mae: 0.0629\n",
      "Epoch 96/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0045 - val_mae: 0.0503\n",
      "Epoch 97/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0051 - val_mae: 0.0546\n",
      "Epoch 98/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0041 - val_mae: 0.0463\n",
      "Epoch 99/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0508 - val_loss: 0.0056 - val_mae: 0.0575\n",
      "Epoch 100/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0043 - val_mae: 0.0485\n",
      "Epoch 101/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0510 - val_loss: 0.0043 - val_mae: 0.0484\n",
      "Epoch 102/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0056 - val_mae: 0.0590\n",
      "Epoch 103/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0044 - val_mae: 0.0486\n",
      "Epoch 104/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0057 - val_mae: 0.0587\n",
      "Epoch 105/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0512 - val_loss: 0.0044 - val_mae: 0.0500\n",
      "Epoch 106/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0050 - val_mae: 0.0537\n",
      "Epoch 107/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0047 - mae: 0.0512 - val_loss: 0.0040 - val_mae: 0.0458\n",
      "Epoch 108/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0046 - val_mae: 0.0506\n",
      "Epoch 109/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0512 - val_loss: 0.0045 - val_mae: 0.0492\n",
      "Epoch 110/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0511 - val_loss: 0.0046 - val_mae: 0.0507\n",
      "Epoch 111/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0511 - val_loss: 0.0064 - val_mae: 0.0624\n",
      "Epoch 112/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0064 - val_mae: 0.0640\n",
      "Epoch 113/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0512 - val_loss: 0.0046 - val_mae: 0.0510\n",
      "Epoch 114/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0042 - val_mae: 0.0476\n",
      "Epoch 115/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0041 - val_mae: 0.0464\n",
      "Epoch 116/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0043 - val_mae: 0.0489\n",
      "Epoch 117/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0043 - val_mae: 0.0475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0062 - val_mae: 0.0606\n",
      "Epoch 119/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0043 - val_mae: 0.0479\n",
      "Epoch 120/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0061 - val_mae: 0.0599\n",
      "Epoch 121/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0059 - val_mae: 0.0591\n",
      "Epoch 122/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0052 - val_mae: 0.0557\n",
      "Epoch 123/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0044 - val_mae: 0.0490\n",
      "Epoch 124/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0054 - val_mae: 0.0548\n",
      "Epoch 125/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0060 - val_mae: 0.0582\n",
      "Epoch 126/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0048 - mae: 0.0516 - val_loss: 0.0047 - val_mae: 0.0508\n",
      "Epoch 127/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0518 - val_loss: 0.0048 - val_mae: 0.0520\n",
      "Epoch 128/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0518 - val_loss: 0.0044 - val_mae: 0.0491\n",
      "Epoch 129/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0045 - val_mae: 0.0498\n",
      "Epoch 130/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0046 - val_mae: 0.0494\n",
      "Epoch 131/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0520 - val_loss: 0.0052 - val_mae: 0.0547\n",
      "Epoch 132/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0045 - val_mae: 0.0500\n",
      "Epoch 133/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0058 - val_mae: 0.0569\n",
      "Epoch 134/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0042 - val_mae: 0.0472\n",
      "Epoch 135/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0049 - mae: 0.0518 - val_loss: 0.0051 - val_mae: 0.0543\n",
      "Epoch 136/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0056 - val_mae: 0.0576\n",
      "Epoch 137/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0042 - val_mae: 0.0471\n",
      "Epoch 138/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0520 - val_loss: 0.0058 - val_mae: 0.0579\n",
      "Epoch 139/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0058 - val_mae: 0.0604\n",
      "Epoch 140/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0050 - mae: 0.0523 - val_loss: 0.0048 - val_mae: 0.0539\n",
      "Epoch 141/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0523 - val_loss: 0.0054 - val_mae: 0.0560\n",
      "Epoch 142/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0524 - val_loss: 0.0059 - val_mae: 0.0620\n",
      "Epoch 143/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0524 - val_loss: 0.0052 - val_mae: 0.0556\n",
      "Epoch 144/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0525 - val_loss: 0.0049 - val_mae: 0.0530\n",
      "Epoch 145/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0050 - mae: 0.0523 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 146/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0524 - val_loss: 0.0054 - val_mae: 0.0577\n",
      "Epoch 147/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0050 - mae: 0.0524 - val_loss: 0.0063 - val_mae: 0.0611\n",
      "Epoch 148/150\n",
      "31364/31364 [==============================] - 33s 1ms/step - loss: 0.0049 - mae: 0.0522 - val_loss: 0.0046 - val_mae: 0.0515\n",
      "Epoch 149/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0525 - val_loss: 0.0053 - val_mae: 0.0556\n",
      "Epoch 150/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0049 - mae: 0.0528 - val_loss: 0.0047 - val_mae: 0.0527\n",
      "processing fold # 1\n",
      "Train on 31364 samples, validate on 7840 samples\n",
      "Epoch 1/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0125 - mae: 0.0845 - val_loss: 0.0097 - val_mae: 0.0732\n",
      "Epoch 2/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0090 - mae: 0.0712 - val_loss: 0.0079 - val_mae: 0.0680\n",
      "Epoch 3/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0079 - mae: 0.0666 - val_loss: 0.0069 - val_mae: 0.0609\n",
      "Epoch 4/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0072 - mae: 0.0635 - val_loss: 0.0066 - val_mae: 0.0617\n",
      "Epoch 5/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0065 - mae: 0.0607 - val_loss: 0.0058 - val_mae: 0.0582\n",
      "Epoch 6/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0584 - val_loss: 0.0069 - val_mae: 0.0651\n",
      "Epoch 7/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0577 - val_loss: 0.0057 - val_mae: 0.0552\n",
      "Epoch 8/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0573 - val_loss: 0.0050 - val_mae: 0.0524\n",
      "Epoch 9/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0568 - val_loss: 0.0059 - val_mae: 0.0563\n",
      "Epoch 10/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0067 - val_mae: 0.0631\n",
      "Epoch 11/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0554 - val_loss: 0.0056 - val_mae: 0.0549\n",
      "Epoch 12/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0047 - val_mae: 0.0499\n",
      "Epoch 13/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0544 - val_loss: 0.0053 - val_mae: 0.0520\n",
      "Epoch 14/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0541 - val_loss: 0.0054 - val_mae: 0.0549\n",
      "Epoch 15/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0051 - val_mae: 0.0518\n",
      "Epoch 16/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0051 - val_mae: 0.0525\n",
      "Epoch 17/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0052 - val_mae: 0.0540\n",
      "Epoch 18/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0051 - val_mae: 0.0518\n",
      "Epoch 19/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0053 - val_mae: 0.0537\n",
      "Epoch 20/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0052 - val_mae: 0.0538\n",
      "Epoch 21/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0049 - val_mae: 0.0523\n",
      "Epoch 22/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0048 - val_mae: 0.0523\n",
      "Epoch 23/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0059 - val_mae: 0.0573\n",
      "Epoch 24/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0047 - val_mae: 0.0512\n",
      "Epoch 25/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0049 - val_mae: 0.0519\n",
      "Epoch 26/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0054 - val_mae: 0.0562\n",
      "Epoch 27/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0543 - val_loss: 0.0055 - val_mae: 0.0534\n",
      "Epoch 28/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0551 - val_loss: 0.0049 - val_mae: 0.0506\n",
      "Epoch 29/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0048 - val_mae: 0.0522\n",
      "Epoch 30/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0054 - val_mae: 0.0541\n",
      "Epoch 31/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0057 - val_mae: 0.0555\n",
      "Epoch 32/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0070 - val_mae: 0.0595\n",
      "Epoch 33/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0550 - val_loss: 0.0052 - val_mae: 0.0525\n",
      "Epoch 34/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0046 - val_mae: 0.0503\n",
      "Epoch 35/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0050 - val_mae: 0.0522\n",
      "Epoch 36/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0074 - val_mae: 0.0677\n",
      "Epoch 37/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0061 - val_mae: 0.0581\n",
      "Epoch 38/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0046 - val_mae: 0.0499\n",
      "Epoch 39/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0053 - val_mae: 0.0537\n",
      "Epoch 40/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0047 - val_mae: 0.0496\n",
      "Epoch 41/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0049 - val_mae: 0.0529\n",
      "Epoch 42/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0055 - val_mae: 0.0542\n",
      "Epoch 43/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0058 - val_mae: 0.0566\n",
      "Epoch 44/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0048 - val_mae: 0.0515\n",
      "Epoch 45/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0057 - val_mae: 0.0572\n",
      "Epoch 46/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0539 - val_loss: 0.0059 - val_mae: 0.0568\n",
      "Epoch 47/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0055 - val_mae: 0.0568\n",
      "Epoch 48/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0053 - val_mae: 0.0551\n",
      "Epoch 49/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0050 - val_mae: 0.0512\n",
      "Epoch 50/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0066 - val_mae: 0.0610\n",
      "Epoch 51/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0057 - val_mae: 0.0581\n",
      "Epoch 52/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0539 - val_loss: 0.0050 - val_mae: 0.0504\n",
      "Epoch 53/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0046 - val_mae: 0.0502\n",
      "Epoch 54/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0080 - val_mae: 0.0724\n",
      "Epoch 55/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0540 - val_loss: 0.0048 - val_mae: 0.0510\n",
      "Epoch 56/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0077 - val_mae: 0.0662\n",
      "Epoch 57/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0048 - val_mae: 0.0523\n",
      "Epoch 58/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0539 - val_loss: 0.0046 - val_mae: 0.0498\n",
      "Epoch 59/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0539 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 60/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0059 - val_mae: 0.0570\n",
      "Epoch 61/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0055 - val_mae: 0.0563\n",
      "Epoch 62/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0045 - val_mae: 0.0494\n",
      "Epoch 63/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0048 - val_mae: 0.0515\n",
      "Epoch 64/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0049 - val_mae: 0.0522\n",
      "Epoch 65/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0540 - val_loss: 0.0061 - val_mae: 0.0607\n",
      "Epoch 66/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0057 - val_mae: 0.0552\n",
      "Epoch 67/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0061 - val_mae: 0.0608\n",
      "Epoch 68/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0055 - val_mae: 0.0543\n",
      "Epoch 69/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0061 - val_mae: 0.0580\n",
      "Epoch 70/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0055 - val_mae: 0.0549\n",
      "Epoch 71/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0045 - val_mae: 0.0496\n",
      "Epoch 72/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 73/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0059 - val_mae: 0.0583\n",
      "Epoch 74/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0055 - val_mae: 0.0546\n",
      "Epoch 75/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0063 - val_mae: 0.0613\n",
      "Epoch 76/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0061 - val_mae: 0.0572\n",
      "Epoch 77/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0546 - val_loss: 0.0071 - val_mae: 0.0625\n",
      "Epoch 78/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0049 - val_mae: 0.0517\n",
      "Epoch 79/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0057 - val_mae: 0.0578\n",
      "Epoch 80/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0051 - val_mae: 0.0528\n",
      "Epoch 81/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0047 - val_mae: 0.0507\n",
      "Epoch 82/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0045 - val_mae: 0.0495\n",
      "Epoch 83/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0045 - val_mae: 0.0498\n",
      "Epoch 84/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0054 - val_mae: 0.0555\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0049 - val_mae: 0.0510\n",
      "Epoch 86/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0051 - val_mae: 0.0523\n",
      "Epoch 87/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0057 - val_mae: 0.0576\n",
      "Epoch 88/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0048 - val_mae: 0.0503\n",
      "Epoch 89/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0071 - val_mae: 0.0618\n",
      "Epoch 90/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0059 - val_mae: 0.0593\n",
      "Epoch 91/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0052 - val_mae: 0.0517\n",
      "Epoch 92/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0069 - val_mae: 0.0649\n",
      "Epoch 93/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0050 - val_mae: 0.0533\n",
      "Epoch 94/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0048 - val_mae: 0.0511\n",
      "Epoch 95/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0047 - val_mae: 0.0496\n",
      "Epoch 96/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0048 - val_mae: 0.0508\n",
      "Epoch 97/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0094 - val_mae: 0.0787\n",
      "Epoch 98/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0061 - val_mae: 0.0600\n",
      "Epoch 99/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0081 - val_mae: 0.0660\n",
      "Epoch 100/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0053 - val_mae: 0.0553\n",
      "Epoch 101/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0053 - val_mae: 0.0539\n",
      "Epoch 102/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0049 - val_mae: 0.0517\n",
      "Epoch 103/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0051 - val_mae: 0.0532\n",
      "Epoch 104/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0056 - val_mae: 0.0576\n",
      "Epoch 105/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0060 - val_mae: 0.0581\n",
      "Epoch 106/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0071 - val_mae: 0.0620\n",
      "Epoch 107/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0057 - val_mae: 0.0577\n",
      "Epoch 108/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0546 - val_loss: 0.0050 - val_mae: 0.0525\n",
      "Epoch 109/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0067 - val_mae: 0.0640\n",
      "Epoch 110/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0056 - val_mae: 0.0559\n",
      "Epoch 111/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0550 - val_loss: 0.0045 - val_mae: 0.0497\n",
      "Epoch 112/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0054 - val_mae: 0.0542\n",
      "Epoch 113/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0556 - val_loss: 0.0052 - val_mae: 0.0552\n",
      "Epoch 114/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0562 - val_loss: 0.0052 - val_mae: 0.0533\n",
      "Epoch 115/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0055 - val_mae: 0.0555\n",
      "Epoch 116/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0566 - val_loss: 0.0057 - val_mae: 0.0565\n",
      "Epoch 117/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0570 - val_loss: 0.0051 - val_mae: 0.0536\n",
      "Epoch 118/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0572 - val_loss: 0.0053 - val_mae: 0.0554\n",
      "Epoch 119/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0571 - val_loss: 0.0056 - val_mae: 0.0565\n",
      "Epoch 120/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0570 - val_loss: 0.0048 - val_mae: 0.0519\n",
      "Epoch 121/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0568 - val_loss: 0.0047 - val_mae: 0.0510\n",
      "Epoch 122/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0570 - val_loss: 0.0051 - val_mae: 0.0527\n",
      "Epoch 123/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0567 - val_loss: 0.0063 - val_mae: 0.0610\n",
      "Epoch 124/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0570 - val_loss: 0.0054 - val_mae: 0.0545\n",
      "Epoch 125/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0567 - val_loss: 0.0064 - val_mae: 0.0593\n",
      "Epoch 126/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0051 - val_mae: 0.0532\n",
      "Epoch 127/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0568 - val_loss: 0.0047 - val_mae: 0.0511\n",
      "Epoch 128/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0568 - val_loss: 0.0047 - val_mae: 0.0506\n",
      "Epoch 129/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0567 - val_loss: 0.0049 - val_mae: 0.0518\n",
      "Epoch 130/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0570 - val_loss: 0.0055 - val_mae: 0.0550\n",
      "Epoch 131/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0572 - val_loss: 0.0078 - val_mae: 0.0718\n",
      "Epoch 132/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0574 - val_loss: 0.0072 - val_mae: 0.0651\n",
      "Epoch 133/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0572 - val_loss: 0.0053 - val_mae: 0.0540\n",
      "Epoch 134/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0571 - val_loss: 0.0049 - val_mae: 0.0523\n",
      "Epoch 135/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0055 - val_mae: 0.0538\n",
      "Epoch 136/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0052 - val_mae: 0.0522\n",
      "Epoch 137/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0048 - val_mae: 0.0520\n",
      "Epoch 138/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0565 - val_loss: 0.0050 - val_mae: 0.0529\n",
      "Epoch 139/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0068 - val_mae: 0.0587\n",
      "Epoch 140/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0567 - val_loss: 0.0050 - val_mae: 0.0513\n",
      "Epoch 141/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0071 - val_mae: 0.0645\n",
      "Epoch 142/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0048 - val_mae: 0.0502\n",
      "Epoch 143/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0563 - val_loss: 0.0047 - val_mae: 0.0507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0563 - val_loss: 0.0075 - val_mae: 0.0636\n",
      "Epoch 145/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0563 - val_loss: 0.0046 - val_mae: 0.0500\n",
      "Epoch 146/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0566 - val_loss: 0.0052 - val_mae: 0.0538\n",
      "Epoch 147/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0050 - val_mae: 0.0522\n",
      "Epoch 148/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0568 - val_loss: 0.0049 - val_mae: 0.0519\n",
      "Epoch 149/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0046 - val_mae: 0.0496\n",
      "Epoch 150/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0564 - val_loss: 0.0071 - val_mae: 0.0656\n",
      "processing fold # 2\n",
      "Train on 31364 samples, validate on 7840 samples\n",
      "Epoch 1/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0120 - mae: 0.0826 - val_loss: 0.0081 - val_mae: 0.0709\n",
      "Epoch 2/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0073 - mae: 0.0645 - val_loss: 0.0067 - val_mae: 0.0623\n",
      "Epoch 3/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0069 - mae: 0.0623 - val_loss: 0.0066 - val_mae: 0.0616\n",
      "Epoch 4/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0066 - mae: 0.0607 - val_loss: 0.0061 - val_mae: 0.0597\n",
      "Epoch 5/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0063 - mae: 0.0596 - val_loss: 0.0059 - val_mae: 0.0592\n",
      "Epoch 6/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0062 - mae: 0.0594 - val_loss: 0.0083 - val_mae: 0.0654\n",
      "Epoch 7/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0589 - val_loss: 0.0056 - val_mae: 0.0553\n",
      "Epoch 8/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0584 - val_loss: 0.0056 - val_mae: 0.0545\n",
      "Epoch 9/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0579 - val_loss: 0.0066 - val_mae: 0.0625\n",
      "Epoch 10/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0580 - val_loss: 0.0051 - val_mae: 0.0533\n",
      "Epoch 11/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0052 - val_mae: 0.0534\n",
      "Epoch 12/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0577 - val_loss: 0.0068 - val_mae: 0.0618\n",
      "Epoch 13/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0059 - mae: 0.0578 - val_loss: 0.0055 - val_mae: 0.0554\n",
      "Epoch 14/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0575 - val_loss: 0.0066 - val_mae: 0.0592\n",
      "Epoch 15/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0579 - val_loss: 0.0063 - val_mae: 0.0613\n",
      "Epoch 16/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0580 - val_loss: 0.0054 - val_mae: 0.0552\n",
      "Epoch 17/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0582 - val_loss: 0.0053 - val_mae: 0.0538\n",
      "Epoch 18/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0060 - mae: 0.0585 - val_loss: 0.0060 - val_mae: 0.0568\n",
      "Epoch 19/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0584 - val_loss: 0.0057 - val_mae: 0.0548\n",
      "Epoch 20/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0585 - val_loss: 0.0055 - val_mae: 0.0565\n",
      "Epoch 21/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0585 - val_loss: 0.0060 - val_mae: 0.0607\n",
      "Epoch 22/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0585 - val_loss: 0.0078 - val_mae: 0.0701\n",
      "Epoch 23/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0588 - val_loss: 0.0059 - val_mae: 0.0570\n",
      "Epoch 24/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0060 - mae: 0.0584 - val_loss: 0.0053 - val_mae: 0.0551\n",
      "Epoch 25/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0582 - val_loss: 0.0056 - val_mae: 0.0552\n",
      "Epoch 26/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0581 - val_loss: 0.0052 - val_mae: 0.0545\n",
      "Epoch 27/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0060 - mae: 0.0578 - val_loss: 0.0053 - val_mae: 0.0537\n",
      "Epoch 28/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0061 - val_mae: 0.0607\n",
      "Epoch 29/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0058 - mae: 0.0572 - val_loss: 0.0053 - val_mae: 0.0545\n",
      "Epoch 30/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0573 - val_loss: 0.0056 - val_mae: 0.0556\n",
      "Epoch 31/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0058 - mae: 0.0569 - val_loss: 0.0052 - val_mae: 0.0541\n",
      "Epoch 32/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0058 - mae: 0.0570 - val_loss: 0.0053 - val_mae: 0.0545\n",
      "Epoch 33/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0567 - val_loss: 0.0052 - val_mae: 0.0532\n",
      "Epoch 34/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0566 - val_loss: 0.0051 - val_mae: 0.0530\n",
      "Epoch 35/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0568 - val_loss: 0.0051 - val_mae: 0.0527\n",
      "Epoch 36/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0564 - val_loss: 0.0055 - val_mae: 0.0561\n",
      "Epoch 37/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0563 - val_loss: 0.0051 - val_mae: 0.0536\n",
      "Epoch 38/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0077 - val_mae: 0.0657\n",
      "Epoch 39/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0055 - val_mae: 0.0558\n",
      "Epoch 40/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0563 - val_loss: 0.0054 - val_mae: 0.0546\n",
      "Epoch 41/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0560 - val_loss: 0.0052 - val_mae: 0.0531\n",
      "Epoch 42/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0559 - val_loss: 0.0054 - val_mae: 0.0560\n",
      "Epoch 43/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0555 - val_loss: 0.0054 - val_mae: 0.0551\n",
      "Epoch 44/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0556 - val_loss: 0.0062 - val_mae: 0.0581\n",
      "Epoch 45/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0051 - val_mae: 0.0535\n",
      "Epoch 46/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0054 - val_mae: 0.0556\n",
      "Epoch 47/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0050 - val_mae: 0.0518\n",
      "Epoch 48/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0054 - val_mae: 0.0546\n",
      "Epoch 49/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0052 - val_mae: 0.0531\n",
      "Epoch 50/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0071 - val_mae: 0.0645\n",
      "Epoch 51/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0055 - val_mae: 0.0573\n",
      "Epoch 52/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0555 - val_loss: 0.0065 - val_mae: 0.0590\n",
      "Epoch 53/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0553 - val_loss: 0.0052 - val_mae: 0.0545\n",
      "Epoch 54/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0553 - val_loss: 0.0053 - val_mae: 0.0546\n",
      "Epoch 55/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0555 - val_loss: 0.0063 - val_mae: 0.0588\n",
      "Epoch 56/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0551 - val_loss: 0.0058 - val_mae: 0.0575\n",
      "Epoch 57/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0550 - val_loss: 0.0051 - val_mae: 0.0530\n",
      "Epoch 58/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0049 - val_mae: 0.0520\n",
      "Epoch 59/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0055 - val_mae: 0.0544\n",
      "Epoch 60/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0062 - val_mae: 0.0625\n",
      "Epoch 61/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0050 - val_mae: 0.0516\n",
      "Epoch 62/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0053 - val_mae: 0.0558\n",
      "Epoch 63/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0057 - val_mae: 0.0558\n",
      "Epoch 64/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0049 - val_mae: 0.0514\n",
      "Epoch 65/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0051 - val_mae: 0.0527\n",
      "Epoch 66/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0049 - val_mae: 0.0520\n",
      "Epoch 67/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0053 - val_mae: 0.0565\n",
      "Epoch 68/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0054 - val_mae: 0.0540\n",
      "Epoch 69/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0054 - mae: 0.0551 - val_loss: 0.0052 - val_mae: 0.0531\n",
      "Epoch 70/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0060 - val_mae: 0.0605\n",
      "Epoch 71/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0063 - val_mae: 0.0603\n",
      "Epoch 72/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0049 - val_mae: 0.0525\n",
      "Epoch 73/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0054 - val_mae: 0.0549\n",
      "Epoch 74/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0052 - val_mae: 0.0534\n",
      "Epoch 75/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0063 - val_mae: 0.0631\n",
      "Epoch 76/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0550 - val_loss: 0.0054 - val_mae: 0.0571\n",
      "Epoch 77/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0051 - val_mae: 0.0532\n",
      "Epoch 78/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0051 - val_mae: 0.0524\n",
      "Epoch 79/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0051 - val_mae: 0.0550\n",
      "Epoch 80/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0050 - val_mae: 0.0528\n",
      "Epoch 81/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0050 - val_mae: 0.0525\n",
      "Epoch 82/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0544 - val_loss: 0.0060 - val_mae: 0.0625\n",
      "Epoch 83/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0546 - val_loss: 0.0051 - val_mae: 0.0525\n",
      "Epoch 84/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0069 - val_mae: 0.0608\n",
      "Epoch 85/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0049 - val_mae: 0.0512\n",
      "Epoch 86/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0052 - val_mae: 0.0526\n",
      "Epoch 87/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0086 - val_mae: 0.0760\n",
      "Epoch 88/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0047 - val_mae: 0.0509\n",
      "Epoch 89/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0048 - val_mae: 0.0517\n",
      "Epoch 90/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0545 - val_loss: 0.0048 - val_mae: 0.0512\n",
      "Epoch 91/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0055 - val_mae: 0.0566\n",
      "Epoch 92/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0055 - val_mae: 0.0570\n",
      "Epoch 93/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0056 - val_mae: 0.0572\n",
      "Epoch 94/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0049 - val_mae: 0.0527\n",
      "Epoch 95/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0049 - val_mae: 0.0522\n",
      "Epoch 96/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0058 - val_mae: 0.0557\n",
      "Epoch 97/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0047 - val_mae: 0.0499\n",
      "Epoch 98/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0055 - val_mae: 0.0538\n",
      "Epoch 99/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0052 - val_mae: 0.0526\n",
      "Epoch 100/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0536 - val_loss: 0.0051 - val_mae: 0.0538\n",
      "Epoch 101/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0067 - val_mae: 0.0664\n",
      "Epoch 102/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0051 - val_mae: 0.0537\n",
      "Epoch 103/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0051 - val_mae: 0.0528\n",
      "Epoch 104/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0062 - val_mae: 0.0618\n",
      "Epoch 105/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0050 - val_mae: 0.0530\n",
      "Epoch 106/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0058 - val_mae: 0.0567\n",
      "Epoch 107/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0047 - val_mae: 0.0494\n",
      "Epoch 108/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0057 - val_mae: 0.0584\n",
      "Epoch 109/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0049 - val_mae: 0.0500\n",
      "Epoch 110/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0048 - val_mae: 0.0504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0047 - val_mae: 0.0505\n",
      "Epoch 112/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0053 - val_mae: 0.0546\n",
      "Epoch 113/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0534 - val_loss: 0.0054 - val_mae: 0.0524\n",
      "Epoch 114/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0532 - val_loss: 0.0050 - val_mae: 0.0523\n",
      "Epoch 115/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0532 - val_loss: 0.0072 - val_mae: 0.0630\n",
      "Epoch 116/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0534 - val_loss: 0.0049 - val_mae: 0.0530\n",
      "Epoch 117/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0060 - val_mae: 0.0576\n",
      "Epoch 118/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0064 - val_mae: 0.0591\n",
      "Epoch 119/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0051 - val_mae: 0.0542\n",
      "Epoch 120/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0049 - val_mae: 0.0505\n",
      "Epoch 121/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0045 - val_mae: 0.0500\n",
      "Epoch 122/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0048 - val_mae: 0.0529\n",
      "Epoch 123/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0052 - mae: 0.0536 - val_loss: 0.0051 - val_mae: 0.0523\n",
      "Epoch 124/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0045 - val_mae: 0.0486\n",
      "Epoch 125/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0080 - val_mae: 0.0685\n",
      "Epoch 126/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0049 - val_mae: 0.0508\n",
      "Epoch 127/150\n",
      "31364/31364 [==============================] - 39s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0050 - val_mae: 0.0506\n",
      "Epoch 128/150\n",
      "31364/31364 [==============================] - 45s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0055 - val_mae: 0.0572\n",
      "Epoch 129/150\n",
      "31364/31364 [==============================] - 41s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0063 - val_mae: 0.0599\n",
      "Epoch 130/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0050 - val_mae: 0.0502\n",
      "Epoch 131/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0543 - val_loss: 0.0053 - val_mae: 0.0543\n",
      "Epoch 132/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 133/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0053 - val_mae: 0.0574\n",
      "Epoch 134/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0067 - val_mae: 0.0630\n",
      "Epoch 135/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0058 - val_mae: 0.0542\n",
      "Epoch 136/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0058 - val_mae: 0.0564\n",
      "Epoch 137/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0045 - val_mae: 0.0499\n",
      "Epoch 138/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0049 - val_mae: 0.0522\n",
      "Epoch 139/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0054 - val_mae: 0.0558\n",
      "Epoch 140/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0060 - val_mae: 0.0587\n",
      "Epoch 141/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0550 - val_loss: 0.0059 - val_mae: 0.0546\n",
      "Epoch 142/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0054 - val_mae: 0.0549\n",
      "Epoch 143/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0049 - val_mae: 0.0528\n",
      "Epoch 144/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0055 - val_mae: 0.0552\n",
      "Epoch 145/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0555 - val_loss: 0.0050 - val_mae: 0.0528\n",
      "Epoch 146/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0051 - val_mae: 0.0534\n",
      "Epoch 147/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0057 - val_mae: 0.0544\n",
      "Epoch 148/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0558 - val_loss: 0.0047 - val_mae: 0.0489\n",
      "Epoch 149/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0559 - val_loss: 0.0048 - val_mae: 0.0517\n",
      "Epoch 150/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0560 - val_loss: 0.0052 - val_mae: 0.0541\n",
      "processing fold # 3\n",
      "Train on 31364 samples, validate on 7840 samples\n",
      "Epoch 1/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0129 - mae: 0.0868 - val_loss: 0.0113 - val_mae: 0.0824\n",
      "Epoch 2/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0112 - mae: 0.0799 - val_loss: 0.0095 - val_mae: 0.0735\n",
      "Epoch 3/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0091 - mae: 0.0726 - val_loss: 0.0085 - val_mae: 0.0658\n",
      "Epoch 4/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0081 - mae: 0.0689 - val_loss: 0.0073 - val_mae: 0.0645\n",
      "Epoch 5/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0078 - mae: 0.0673 - val_loss: 0.0070 - val_mae: 0.0626\n",
      "Epoch 6/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0076 - mae: 0.0666 - val_loss: 0.0066 - val_mae: 0.0602\n",
      "Epoch 7/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0070 - mae: 0.0636 - val_loss: 0.0069 - val_mae: 0.0648\n",
      "Epoch 8/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0067 - mae: 0.0617 - val_loss: 0.0064 - val_mae: 0.0603\n",
      "Epoch 9/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0065 - mae: 0.0608 - val_loss: 0.0065 - val_mae: 0.0576\n",
      "Epoch 10/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0064 - mae: 0.0604 - val_loss: 0.0063 - val_mae: 0.0578\n",
      "Epoch 11/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0063 - mae: 0.0596 - val_loss: 0.0056 - val_mae: 0.0562\n",
      "Epoch 12/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0062 - mae: 0.0588 - val_loss: 0.0108 - val_mae: 0.0805\n",
      "Epoch 13/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0588 - val_loss: 0.0056 - val_mae: 0.0542\n",
      "Epoch 14/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0588 - val_loss: 0.0059 - val_mae: 0.0579\n",
      "Epoch 15/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0588 - val_loss: 0.0080 - val_mae: 0.0668\n",
      "Epoch 16/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0062 - mae: 0.0592 - val_loss: 0.0062 - val_mae: 0.0573\n",
      "Epoch 17/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0063 - mae: 0.0595 - val_loss: 0.0062 - val_mae: 0.0605\n",
      "Epoch 18/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0590 - val_loss: 0.0079 - val_mae: 0.0673\n",
      "Epoch 19/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0058 - mae: 0.0577 - val_loss: 0.0053 - val_mae: 0.0548\n",
      "Epoch 20/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0562 - val_loss: 0.0048 - val_mae: 0.0513\n",
      "Epoch 21/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0554 - val_loss: 0.0050 - val_mae: 0.0515\n",
      "Epoch 22/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0551 - val_loss: 0.0048 - val_mae: 0.0517\n",
      "Epoch 23/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0549 - val_loss: 0.0057 - val_mae: 0.0587\n",
      "Epoch 24/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0054 - mae: 0.0551 - val_loss: 0.0053 - val_mae: 0.0559\n",
      "Epoch 25/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0552 - val_loss: 0.0051 - val_mae: 0.0531\n",
      "Epoch 26/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0559 - val_loss: 0.0058 - val_mae: 0.0591\n",
      "Epoch 27/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0564 - val_loss: 0.0053 - val_mae: 0.0545\n",
      "Epoch 28/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0567 - val_loss: 0.0055 - val_mae: 0.0553\n",
      "Epoch 29/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0568 - val_loss: 0.0056 - val_mae: 0.0570\n",
      "Epoch 30/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0058 - mae: 0.0571 - val_loss: 0.0050 - val_mae: 0.0520\n",
      "Epoch 31/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0564 - val_loss: 0.0052 - val_mae: 0.0535\n",
      "Epoch 32/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0560 - val_loss: 0.0049 - val_mae: 0.0518\n",
      "Epoch 33/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0556 - val_loss: 0.0054 - val_mae: 0.0549\n",
      "Epoch 34/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0557 - val_loss: 0.0056 - val_mae: 0.0558\n",
      "Epoch 35/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0557 - val_loss: 0.0049 - val_mae: 0.0512\n",
      "Epoch 36/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0066 - val_mae: 0.0591\n",
      "Epoch 37/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0547 - val_loss: 0.0054 - val_mae: 0.0565\n",
      "Epoch 38/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0089 - val_mae: 0.0711\n",
      "Epoch 39/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0545 - val_loss: 0.0054 - val_mae: 0.0534\n",
      "Epoch 40/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0545 - val_loss: 0.0048 - val_mae: 0.0501\n",
      "Epoch 41/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0080 - val_mae: 0.0685\n",
      "Epoch 42/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0547 - val_loss: 0.0051 - val_mae: 0.0535\n",
      "Epoch 43/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0065 - val_mae: 0.0587\n",
      "Epoch 44/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0049 - val_mae: 0.0519\n",
      "Epoch 45/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0053 - mae: 0.0545 - val_loss: 0.0054 - val_mae: 0.0536\n",
      "Epoch 46/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0053 - val_mae: 0.0543\n",
      "Epoch 47/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0049 - val_mae: 0.0512\n",
      "Epoch 48/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0550 - val_loss: 0.0069 - val_mae: 0.0631\n",
      "Epoch 49/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0546 - val_loss: 0.0058 - val_mae: 0.0584\n",
      "Epoch 50/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0050 - val_mae: 0.0514\n",
      "Epoch 51/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0050 - val_mae: 0.0513\n",
      "Epoch 52/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0055 - val_mae: 0.0557\n",
      "Epoch 53/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0057 - val_mae: 0.0554\n",
      "Epoch 54/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0055 - val_mae: 0.0544\n",
      "Epoch 55/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0541 - val_loss: 0.0053 - val_mae: 0.0544\n",
      "Epoch 56/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0537 - val_loss: 0.0052 - val_mae: 0.0526\n",
      "Epoch 57/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0543 - val_loss: 0.0049 - val_mae: 0.0517\n",
      "Epoch 58/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0051 - val_mae: 0.0526\n",
      "Epoch 59/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0055 - val_mae: 0.0554\n",
      "Epoch 60/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0077 - val_mae: 0.0657\n",
      "Epoch 61/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0060 - val_mae: 0.0566\n",
      "Epoch 62/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0538 - val_loss: 0.0053 - val_mae: 0.0554\n",
      "Epoch 63/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0061 - val_mae: 0.0606\n",
      "Epoch 64/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0055 - val_mae: 0.0546\n",
      "Epoch 65/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0052 - val_mae: 0.0527\n",
      "Epoch 66/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0051 - val_mae: 0.0534\n",
      "Epoch 67/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0069 - val_mae: 0.0645\n",
      "Epoch 68/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0537 - val_loss: 0.0056 - val_mae: 0.0571\n",
      "Epoch 69/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0056 - val_mae: 0.0547\n",
      "Epoch 70/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0052 - mae: 0.0539 - val_loss: 0.0058 - val_mae: 0.0581\n",
      "Epoch 71/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0055 - val_mae: 0.0540\n",
      "Epoch 72/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0544 - val_loss: 0.0054 - val_mae: 0.0552\n",
      "Epoch 73/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0048 - val_mae: 0.0507\n",
      "Epoch 74/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0543 - val_loss: 0.0050 - val_mae: 0.0516\n",
      "Epoch 75/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0054 - mae: 0.0547 - val_loss: 0.0048 - val_mae: 0.0503\n",
      "Epoch 76/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0054 - mae: 0.0549 - val_loss: 0.0057 - val_mae: 0.0579\n",
      "Epoch 77/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0054 - val_mae: 0.0549\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0552 - val_loss: 0.0056 - val_mae: 0.0540\n",
      "Epoch 79/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0551 - val_loss: 0.0050 - val_mae: 0.0515\n",
      "Epoch 80/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0552 - val_loss: 0.0054 - val_mae: 0.0532\n",
      "Epoch 81/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0555 - val_loss: 0.0055 - val_mae: 0.0531\n",
      "Epoch 82/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0555 - val_loss: 0.0061 - val_mae: 0.0568\n",
      "Epoch 83/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0053 - val_mae: 0.0536\n",
      "Epoch 84/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0557 - val_loss: 0.0054 - val_mae: 0.0533\n",
      "Epoch 85/150\n",
      "31364/31364 [==============================] - 47s 2ms/step - loss: 0.0056 - mae: 0.0560 - val_loss: 0.0054 - val_mae: 0.0548\n",
      "Epoch 86/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0056 - mae: 0.0557 - val_loss: 0.0068 - val_mae: 0.0642\n",
      "Epoch 87/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0560 - val_loss: 0.0075 - val_mae: 0.0646\n",
      "Epoch 88/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0559 - val_loss: 0.0056 - val_mae: 0.0562\n",
      "Epoch 89/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0560 - val_loss: 0.0057 - val_mae: 0.0570\n",
      "Epoch 90/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0560 - val_loss: 0.0059 - val_mae: 0.0584\n",
      "Epoch 91/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0561 - val_loss: 0.0055 - val_mae: 0.0543\n",
      "Epoch 92/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0562 - val_loss: 0.0055 - val_mae: 0.0547\n",
      "Epoch 93/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0564 - val_loss: 0.0056 - val_mae: 0.0555\n",
      "Epoch 94/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0566 - val_loss: 0.0054 - val_mae: 0.0544\n",
      "Epoch 95/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0056 - val_mae: 0.0560\n",
      "Epoch 96/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0567 - val_loss: 0.0054 - val_mae: 0.0542\n",
      "Epoch 97/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0567 - val_loss: 0.0054 - val_mae: 0.0538\n",
      "Epoch 98/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0566 - val_loss: 0.0079 - val_mae: 0.0676\n",
      "Epoch 99/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0563 - val_loss: 0.0061 - val_mae: 0.0595\n",
      "Epoch 100/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0055 - val_mae: 0.0542\n",
      "Epoch 101/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0554 - val_loss: 0.0058 - val_mae: 0.0552\n",
      "Epoch 102/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0552 - val_loss: 0.0050 - val_mae: 0.0508\n",
      "Epoch 103/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0549 - val_loss: 0.0053 - val_mae: 0.0533\n",
      "Epoch 104/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0551 - val_loss: 0.0054 - val_mae: 0.0521\n",
      "Epoch 105/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0055 - mae: 0.0552 - val_loss: 0.0060 - val_mae: 0.0594\n",
      "Epoch 106/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0553 - val_loss: 0.0054 - val_mae: 0.0541\n",
      "Epoch 107/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0056 - mae: 0.0556 - val_loss: 0.0066 - val_mae: 0.0620\n",
      "Epoch 108/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0557 - val_loss: 0.0055 - val_mae: 0.0548\n",
      "Epoch 109/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0557 - val_loss: 0.0052 - val_mae: 0.0529\n",
      "Epoch 110/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0057 - mae: 0.0560 - val_loss: 0.0061 - val_mae: 0.0598\n",
      "Epoch 111/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0562 - val_loss: 0.0053 - val_mae: 0.0526\n",
      "Epoch 112/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0566 - val_loss: 0.0060 - val_mae: 0.0594\n",
      "Epoch 113/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0568 - val_loss: 0.0064 - val_mae: 0.0587\n",
      "Epoch 114/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0570 - val_loss: 0.0056 - val_mae: 0.0549\n",
      "Epoch 115/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0569 - val_loss: 0.0064 - val_mae: 0.0588\n",
      "Epoch 116/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0571 - val_loss: 0.0053 - val_mae: 0.0510\n",
      "Epoch 117/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0575 - val_loss: 0.0066 - val_mae: 0.0573\n",
      "Epoch 118/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0576 - val_loss: 0.0061 - val_mae: 0.0595\n",
      "Epoch 119/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0062 - mae: 0.0579 - val_loss: 0.0062 - val_mae: 0.0583\n",
      "Epoch 120/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0579 - val_loss: 0.0056 - val_mae: 0.0542\n",
      "Epoch 121/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0062 - mae: 0.0581 - val_loss: 0.0071 - val_mae: 0.0603\n",
      "Epoch 122/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0062 - mae: 0.0582 - val_loss: 0.0063 - val_mae: 0.0557\n",
      "Epoch 123/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0063 - mae: 0.0585 - val_loss: 0.0100 - val_mae: 0.0817\n",
      "Epoch 124/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0063 - mae: 0.0587 - val_loss: 0.0064 - val_mae: 0.0578\n",
      "Epoch 125/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0063 - mae: 0.0582 - val_loss: 0.0058 - val_mae: 0.0534\n",
      "Epoch 126/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0063 - mae: 0.0586 - val_loss: 0.0067 - val_mae: 0.0613\n",
      "Epoch 127/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0062 - mae: 0.0585 - val_loss: 0.0059 - val_mae: 0.0577\n",
      "Epoch 128/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0582 - val_loss: 0.0062 - val_mae: 0.0589\n",
      "Epoch 129/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0581 - val_loss: 0.0063 - val_mae: 0.0597\n",
      "Epoch 130/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0061 - mae: 0.0579 - val_loss: 0.0064 - val_mae: 0.0595\n",
      "Epoch 131/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0576 - val_loss: 0.0068 - val_mae: 0.0619\n",
      "Epoch 132/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0060 - mae: 0.0576 - val_loss: 0.0055 - val_mae: 0.0540\n",
      "Epoch 133/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0060 - mae: 0.0577 - val_loss: 0.0056 - val_mae: 0.0554\n",
      "Epoch 134/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0577 - val_loss: 0.0060 - val_mae: 0.0580\n",
      "Epoch 135/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0087 - val_mae: 0.0726\n",
      "Epoch 136/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0575 - val_loss: 0.0058 - val_mae: 0.0558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0576 - val_loss: 0.0055 - val_mae: 0.0539\n",
      "Epoch 138/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0054 - val_mae: 0.0541\n",
      "Epoch 139/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0063 - val_mae: 0.0576\n",
      "Epoch 140/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0064 - val_mae: 0.0608\n",
      "Epoch 141/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0575 - val_loss: 0.0062 - val_mae: 0.0589\n",
      "Epoch 142/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0574 - val_loss: 0.0062 - val_mae: 0.0590\n",
      "Epoch 143/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0574 - val_loss: 0.0053 - val_mae: 0.0541\n",
      "Epoch 144/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0577 - val_loss: 0.0068 - val_mae: 0.0602\n",
      "Epoch 145/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0577 - val_loss: 0.0052 - val_mae: 0.0537\n",
      "Epoch 146/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0058 - mae: 0.0577 - val_loss: 0.0072 - val_mae: 0.0678\n",
      "Epoch 147/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0056 - val_mae: 0.0560\n",
      "Epoch 148/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0055 - val_mae: 0.0538\n",
      "Epoch 149/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0579 - val_loss: 0.0057 - val_mae: 0.0566\n",
      "Epoch 150/150\n",
      "31364/31364 [==============================] - 34s 1ms/step - loss: 0.0059 - mae: 0.0579 - val_loss: 0.0056 - val_mae: 0.0552\n",
      "processing fold # 4\n",
      "Train on 31364 samples, validate on 7840 samples\n",
      "Epoch 1/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0109 - mae: 0.0784 - val_loss: 0.0061 - val_mae: 0.0576\n",
      "Epoch 2/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0067 - mae: 0.0612 - val_loss: 0.0054 - val_mae: 0.0544\n",
      "Epoch 3/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0063 - mae: 0.0594 - val_loss: 0.0057 - val_mae: 0.0584\n",
      "Epoch 4/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0061 - mae: 0.0587 - val_loss: 0.0087 - val_mae: 0.0695\n",
      "Epoch 5/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0060 - mae: 0.0580 - val_loss: 0.0054 - val_mae: 0.0551\n",
      "Epoch 6/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0059 - mae: 0.0576 - val_loss: 0.0056 - val_mae: 0.0562\n",
      "Epoch 7/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0058 - mae: 0.0568 - val_loss: 0.0056 - val_mae: 0.0555\n",
      "Epoch 8/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0057 - mae: 0.0562 - val_loss: 0.0066 - val_mae: 0.0614\n",
      "Epoch 9/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0559 - val_loss: 0.0060 - val_mae: 0.0606\n",
      "Epoch 10/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0056 - mae: 0.0555 - val_loss: 0.0050 - val_mae: 0.0518\n",
      "Epoch 11/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0553 - val_loss: 0.0050 - val_mae: 0.0517\n",
      "Epoch 12/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0550 - val_loss: 0.0047 - val_mae: 0.0500\n",
      "Epoch 13/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0051 - val_mae: 0.0520\n",
      "Epoch 14/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0549 - val_loss: 0.0062 - val_mae: 0.0588\n",
      "Epoch 15/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0549 - val_loss: 0.0049 - val_mae: 0.0528\n",
      "Epoch 16/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0549 - val_loss: 0.0055 - val_mae: 0.0551\n",
      "Epoch 17/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0047 - val_mae: 0.0510\n",
      "Epoch 18/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0071 - val_mae: 0.0664\n",
      "Epoch 19/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0050 - val_mae: 0.0517\n",
      "Epoch 20/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0544 - val_loss: 0.0051 - val_mae: 0.0532\n",
      "Epoch 21/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0048 - val_mae: 0.0502\n",
      "Epoch 22/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0547 - val_loss: 0.0050 - val_mae: 0.0527\n",
      "Epoch 23/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0055 - mae: 0.0549 - val_loss: 0.0049 - val_mae: 0.0516\n",
      "Epoch 24/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0048 - val_mae: 0.0510\n",
      "Epoch 25/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0543 - val_loss: 0.0066 - val_mae: 0.0642\n",
      "Epoch 26/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0054 - mae: 0.0543 - val_loss: 0.0054 - val_mae: 0.0552\n",
      "Epoch 27/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0053 - mae: 0.0539 - val_loss: 0.0052 - val_mae: 0.0542\n",
      "Epoch 28/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0532 - val_loss: 0.0046 - val_mae: 0.0494\n",
      "Epoch 29/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0046 - val_mae: 0.0494\n",
      "Epoch 30/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0524 - val_loss: 0.0047 - val_mae: 0.0500\n",
      "Epoch 31/150\n",
      "31364/31364 [==============================] - 36s 1ms/step - loss: 0.0050 - mae: 0.0526 - val_loss: 0.0052 - val_mae: 0.0533\n",
      "Epoch 32/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0524 - val_loss: 0.0054 - val_mae: 0.0542\n",
      "Epoch 33/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0046 - val_mae: 0.0497\n",
      "Epoch 34/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0524 - val_loss: 0.0049 - val_mae: 0.0523\n",
      "Epoch 35/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0520 - val_loss: 0.0046 - val_mae: 0.0505\n",
      "Epoch 36/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0520 - val_loss: 0.0044 - val_mae: 0.0483\n",
      "Epoch 37/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0518 - val_loss: 0.0054 - val_mae: 0.0538\n",
      "Epoch 38/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0065 - val_mae: 0.0638\n",
      "Epoch 39/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0519 - val_loss: 0.0044 - val_mae: 0.0482\n",
      "Epoch 40/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0045 - val_mae: 0.0490\n",
      "Epoch 41/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0523 - val_loss: 0.0046 - val_mae: 0.0497\n",
      "Epoch 42/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0521 - val_loss: 0.0043 - val_mae: 0.0480\n",
      "Epoch 43/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0521 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 44/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0523 - val_loss: 0.0069 - val_mae: 0.0651\n",
      "Epoch 45/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0527 - val_loss: 0.0045 - val_mae: 0.0490\n",
      "Epoch 46/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0525 - val_loss: 0.0046 - val_mae: 0.0505\n",
      "Epoch 47/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0044 - val_mae: 0.0488\n",
      "Epoch 48/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0052 - val_mae: 0.0524\n",
      "Epoch 49/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0531 - val_loss: 0.0053 - val_mae: 0.0538\n",
      "Epoch 50/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0052 - mae: 0.0531 - val_loss: 0.0049 - val_mae: 0.0512\n",
      "Epoch 51/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0047 - val_mae: 0.0510\n",
      "Epoch 52/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0518 - val_loss: 0.0043 - val_mae: 0.0483\n",
      "Epoch 53/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0516 - val_loss: 0.0049 - val_mae: 0.0528\n",
      "Epoch 54/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0057 - val_mae: 0.0564\n",
      "Epoch 55/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0518 - val_loss: 0.0046 - val_mae: 0.0508\n",
      "Epoch 56/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0517 - val_loss: 0.0045 - val_mae: 0.0491\n",
      "Epoch 57/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0519 - val_loss: 0.0053 - val_mae: 0.0539\n",
      "Epoch 58/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0522 - val_loss: 0.0052 - val_mae: 0.0534\n",
      "Epoch 59/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0043 - val_mae: 0.0481\n",
      "Epoch 60/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0043 - val_mae: 0.0475\n",
      "Epoch 61/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0520 - val_loss: 0.0049 - val_mae: 0.0531\n",
      "Epoch 62/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0047 - val_mae: 0.0508\n",
      "Epoch 63/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0518 - val_loss: 0.0048 - val_mae: 0.0510\n",
      "Epoch 64/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0048 - val_mae: 0.0511\n",
      "Epoch 65/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0043 - val_mae: 0.0485\n",
      "Epoch 66/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0050 - val_mae: 0.0529\n",
      "Epoch 67/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0049 - val_mae: 0.0515\n",
      "Epoch 68/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0042 - val_mae: 0.0476\n",
      "Epoch 69/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0043 - val_mae: 0.0488\n",
      "Epoch 70/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0510 - val_loss: 0.0045 - val_mae: 0.0493\n",
      "Epoch 71/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0509 - val_loss: 0.0042 - val_mae: 0.0471\n",
      "Epoch 72/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0056 - val_mae: 0.0544\n",
      "Epoch 73/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0054 - val_mae: 0.0541\n",
      "Epoch 74/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0048 - val_mae: 0.0518\n",
      "Epoch 75/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0047 - val_mae: 0.0506\n",
      "Epoch 76/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0515 - val_loss: 0.0042 - val_mae: 0.0481\n",
      "Epoch 77/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0520 - val_loss: 0.0047 - val_mae: 0.0504\n",
      "Epoch 78/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0046 - val_mae: 0.0494\n",
      "Epoch 79/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0050 - val_mae: 0.0521\n",
      "Epoch 80/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0520 - val_loss: 0.0046 - val_mae: 0.0510\n",
      "Epoch 81/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0523 - val_loss: 0.0052 - val_mae: 0.0546\n",
      "Epoch 82/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0043 - val_mae: 0.0486\n",
      "Epoch 83/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0062 - val_mae: 0.0604\n",
      "Epoch 84/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0049 - val_mae: 0.0541\n",
      "Epoch 85/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0523 - val_loss: 0.0050 - val_mae: 0.0517\n",
      "Epoch 86/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0522 - val_loss: 0.0050 - val_mae: 0.0544\n",
      "Epoch 87/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0049 - mae: 0.0523 - val_loss: 0.0045 - val_mae: 0.0500\n",
      "Epoch 88/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0525 - val_loss: 0.0048 - val_mae: 0.0503\n",
      "Epoch 89/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0525 - val_loss: 0.0043 - val_mae: 0.0485\n",
      "Epoch 90/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0525 - val_loss: 0.0048 - val_mae: 0.0522\n",
      "Epoch 91/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0526 - val_loss: 0.0065 - val_mae: 0.0599\n",
      "Epoch 92/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0524 - val_loss: 0.0057 - val_mae: 0.0564\n",
      "Epoch 93/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0060 - val_mae: 0.0559\n",
      "Epoch 94/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0045 - val_mae: 0.0498\n",
      "Epoch 95/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0531 - val_loss: 0.0056 - val_mae: 0.0565\n",
      "Epoch 96/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0528 - val_loss: 0.0044 - val_mae: 0.0489\n",
      "Epoch 97/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0055 - val_mae: 0.0559\n",
      "Epoch 98/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0531 - val_loss: 0.0048 - val_mae: 0.0524\n",
      "Epoch 99/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0045 - val_mae: 0.0486\n",
      "Epoch 100/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0044 - val_mae: 0.0491\n",
      "Epoch 101/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0047 - val_mae: 0.0497\n",
      "Epoch 102/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0534 - val_loss: 0.0044 - val_mae: 0.0493\n",
      "Epoch 103/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0534 - val_loss: 0.0052 - val_mae: 0.0525\n",
      "Epoch 104/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0058 - val_mae: 0.0596\n",
      "Epoch 105/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0531 - val_loss: 0.0051 - val_mae: 0.0548\n",
      "Epoch 106/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0051 - val_mae: 0.0536\n",
      "Epoch 107/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0535 - val_loss: 0.0049 - val_mae: 0.0525\n",
      "Epoch 108/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0531 - val_loss: 0.0046 - val_mae: 0.0499\n",
      "Epoch 109/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0056 - val_mae: 0.0561\n",
      "Epoch 110/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0527 - val_loss: 0.0049 - val_mae: 0.0536\n",
      "Epoch 111/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0059 - val_mae: 0.0575\n",
      "Epoch 112/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0054 - val_mae: 0.0571\n",
      "Epoch 113/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0047 - val_mae: 0.0523\n",
      "Epoch 114/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0527 - val_loss: 0.0045 - val_mae: 0.0495\n",
      "Epoch 115/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0043 - val_mae: 0.0484\n",
      "Epoch 116/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0526 - val_loss: 0.0048 - val_mae: 0.0504\n",
      "Epoch 117/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0527 - val_loss: 0.0046 - val_mae: 0.0512\n",
      "Epoch 118/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0045 - val_mae: 0.0505\n",
      "Epoch 119/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0527 - val_loss: 0.0042 - val_mae: 0.0476\n",
      "Epoch 120/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0529 - val_loss: 0.0046 - val_mae: 0.0501\n",
      "Epoch 121/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0050 - val_mae: 0.0523\n",
      "Epoch 122/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0051 - val_mae: 0.0544\n",
      "Epoch 123/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0046 - val_mae: 0.0510\n",
      "Epoch 124/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0066 - val_mae: 0.0648\n",
      "Epoch 125/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0045 - val_mae: 0.0500\n",
      "Epoch 126/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0047 - val_mae: 0.0496\n",
      "Epoch 127/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0044 - val_mae: 0.0492\n",
      "Epoch 128/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0047 - val_mae: 0.0505\n",
      "Epoch 129/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0047 - val_mae: 0.0515\n",
      "Epoch 130/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0062 - val_mae: 0.0596\n",
      "Epoch 131/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0527 - val_loss: 0.0046 - val_mae: 0.0507\n",
      "Epoch 132/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0527 - val_loss: 0.0070 - val_mae: 0.0641\n",
      "Epoch 133/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0045 - val_mae: 0.0502\n",
      "Epoch 134/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0050 - val_mae: 0.0529\n",
      "Epoch 135/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0531 - val_loss: 0.0044 - val_mae: 0.0491\n",
      "Epoch 136/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0053 - val_mae: 0.0541\n",
      "Epoch 137/150\n",
      "31364/31364 [==============================] - 37s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0043 - val_mae: 0.0488\n",
      "Epoch 138/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0051 - val_mae: 0.0555\n",
      "Epoch 139/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0049 - val_mae: 0.0516\n",
      "Epoch 140/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0531 - val_loss: 0.0054 - val_mae: 0.0546\n",
      "Epoch 141/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0043 - val_mae: 0.0480\n",
      "Epoch 142/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0055 - val_mae: 0.0543\n",
      "Epoch 143/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0532 - val_loss: 0.0052 - val_mae: 0.0528\n",
      "Epoch 144/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0041 - val_mae: 0.0469\n",
      "Epoch 145/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0045 - val_mae: 0.0485\n",
      "Epoch 146/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0528 - val_loss: 0.0058 - val_mae: 0.0559\n",
      "Epoch 147/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0051 - val_mae: 0.0553\n",
      "Epoch 148/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0528 - val_loss: 0.0050 - val_mae: 0.0538\n",
      "Epoch 149/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0050 - mae: 0.0528 - val_loss: 0.0047 - val_mae: 0.0500\n",
      "Epoch 150/150\n",
      "31364/31364 [==============================] - 35s 1ms/step - loss: 0.0051 - mae: 0.0529 - val_loss: 0.0043 - val_mae: 0.0478\n"
     ]
    }
   ],
   "source": [
    "histories, nmse = k_fold('sensor_1', 150, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSE: \n",
      "0.27307531306958077\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"NMSE: \")\n",
    "print(np.mean(nmse))\n",
    "\n",
    "num_epochs = 150\n",
    "val_mae_history = [np.mean([x['val_mae'][i] for x in histories]) for i in range(num_epochs)]\n",
    "mae_history = [np.mean([x['mae'][i] for x in histories]) for i in range(num_epochs)]\n",
    "plt.plot(range(3, len(val_mae_history) + 1), val_mae_history[2:], 'ro')\n",
    "plt.plot(range(3, len(mae_history) + 1), mae_history[2:], 'bo')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['test', 'train'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss_history = [np.mean([x['val_loss'][i] for x in histories]) for i in range(num_epochs)]\n",
    "loss_history = [np.mean([x['loss'][i] for x in histories]) for i in range(num_epochs)]\n",
    "plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, 'ro')\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, 'bo')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('LOSS')\n",
    "plt.legend(['test', 'train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "history = model.fit(inputs, targets[['sensor_1']], epochs=75, batch_size=1, verbose=1)\n",
    "history.history['mae']\n",
    "model.save(\"nn_sensor_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15995104286519056116\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11722500253068193238\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow           2.1.0       /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages pip      \r\n",
      "tensorflow-estimator 2.1.0       /Users/sebastiangerard/Library/Python/3.7/lib/python/site-packages pip      \r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/35/55/a0dbd642e68e68f3e309d1413abdc0a7aa7e1534c79c0fc2501defb864ac/tensorflow-2.1.0-cp37-cp37m-macosx_10_11_x86_64.whl\n",
      "Collecting keras-preprocessing>=1.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\" (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/85/7a/ae480be23b768910a9327c33517ced4623ba88dc035f9ce0206657c353a9/scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.33.1)\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.18.0)\n",
      "Collecting keras-applications>=1.0.8 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.7.0)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.7.1)\n",
      "Collecting protobuf>=3.8.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/25/c057a298635d08d087a20f51ff4287d821814208ebb045d84ea65535b3e3/protobuf-3.11.3-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/72/1c1498c1e908e0562b1e1cd30012580baa7d33b5b0ffdbeb5fde2462cc71/setuptools-45.2.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/5a/8d/e2ebbd0502627ed0d8a408162020e1c0792f088b49fddeedaaeebc206ed7/google_auth-1.11.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.1)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "\u001b[31mtensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.18.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-preprocessing, opt-einsum, scipy, wrapt, keras-applications, setuptools, protobuf, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, google-pasta, tensorflow-estimator, tensorflow\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Users/sebastiangerard/Library/Python/3.7'\n",
      "Check the permissions.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
