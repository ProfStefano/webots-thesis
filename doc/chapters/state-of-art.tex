\section{Webots}
Webots was created by Cyberbotics Ltd. a spin-off company from the EPFL. It has been developing it since 1998. The company currently\footnote{2019} employs 6 people in Lausanne, Switzerland to continuously develop Webots according to customers needs. Cyberbotics provides consulting on both industrial and academic research projects and delivers open-source software solutions to its customers. It also provides user support and training to the users of the Webots software. The source code and binary packages are available for free; however, user support and consultancy are not. It is available for Windows, Ubuntu Linux and MacOS \cite{cyberbotics}.

Webots website has a reference manual that describes nodes and API functions. It has a complete user guide as well endowed with examples of simulations that show the use of actuators, creation of different environments, geometries primitives, complex behaviors, functionalities and advanced 3D rendering capabilities. This section is oriented to describe the basics of Webots and is focused on what will be useful to develop the project. For a detailed description please refer to the user guide\footnote{\url{https://cyberbotics.com/doc/guide/index}} or the reference manual\footnote{\url{https://cyberbotics.com/doc/reference/index}}.

\subsection{Graphic Interface}
Webots supports the following programming languages: C, C++, Python, Java, MATLAB and ROS. Additionally, it offers the possibility of creating a custom interface to third-party software such as Lisp\textsuperscript{TM} or LabView\textsuperscript{TM} using TCP/IP protocol.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{\images/chapter2/interface.png}
	\caption{Webots graphic interface}
	\label{fig-ch-2:interface}
\end{figure}

Figure \ref{fig-ch-2:interface} shows the main graphic interface of Webots. It can be divided in 5 panels:

\begin{enumerate}
	\item Simulation: graphic visualization of the simulated world objects.
	\item Code visualization: robot controller code editor.
	\item World Information: information represented in a tree structure about the simulated world including the robot components (sensors, actuators, physical characteristics), world objects, textures, etc. 
	\item Console: program execution output stream.
	\item Control panel: set of buttons that controls the simulation execution.
\end{enumerate}

The program allows users to create highly personalized simulated environments which are called worlds, from scratch using pre-built 3D object models such as robots, wood boxes, walls, arenas, etc. A robot needs to be associated with a controller program that contains the source code with the desired behavior. This controller can be easily edited in the code panel and once it is saved it is automatically reloaded into all the robots associated with it. For running the simulation, users can play, stop or reset it, among other options, using the control panel set of buttons. The output stream of the controller execution will be displayed in the console panel. 


\subsection{Webots with Python 3.7 and TensorFlow}

Python was created in 1990 by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands as a successor of a language called ABC\cite{python-docs}. Nowadays it has became one of the most popular programming languages for data science\cite{Raschka:2015:PML:2886323}.

The Python API of Webots was created from C++ API and it supports Python 3.7. It is possible to configure Webots to use Python 3.7 which should be previously installed from the Python website\footnote{Download it from \url{https://www.python.org}}; however, Webots does not work properly with Python versions installed from package managers as Brew. By default Webots is configured to use the default installed version of Python. In order to use another version, access to the menu options in Webots: \path{Webots/Preferences}; the \textit{Python command} label should point out to the installation path of Python3.7 as it is shown in figure \ref{fig-ch-2:python}. 

\begin{figure}[ht]
	\includegraphics[width=\linewidth]{\images/chapter2/preferences.png}
	\caption{Configure Python3.7}
	\label{fig-ch-2:python}
\end{figure}

Python allows to install third-party libraries like TensorFlow which is a Python-friendly open-source library developed by the researchers and engineers of the Google Brain team for internal use only and  then released in November 2015 under a permissive open source license. It implements machine learning algorithms and deep learning wrappers\cite{Raschka:2015:PML:2886323}.

The \verb|pip3| command allows to install any library for Python 3.7. For installing TensorFlow type \verb|pip3 install tensorflow| in the console terminal. For verifying the correct installation, the code displayed in listing \ref{code:verify-tf} can be put inside a controller application in Webots.

\begin{lstlisting}[language=Python, caption=Verify correct installation of TensorFlow, label=code:verify-tf]
import tensorflow as tf

verifier = tf.constant('TensorFlow was installed correctly.')
sess = tf.Session()
print(sess.run(verifier))
\end{lstlisting}

If TensorFlow is correctly installed, after the execution of the simulation, a message in the console panel will be displayed informing about its correct installation.


\subsection{Robots and world creation}

Webots allows creating large simulated worlds. The world description and content is presented as a tree structure where each node represents an object in the world, those objects have themselves nodes and sub-nodes within a name and a value indicating different physical characteristics or components. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{\images/chapter2/nodes.png}
  \caption{World structure}
  \label{fig:ch-2:nodes}
\end{figure}

For adding a node into the world we can press the \verb|Add object| button that will open the window shown in figure \ref{fig:ch-2:nodes}. The base nodes are the basic objects that can be part of the world. Moreover, they are simpler models than the others. The USE nodes are objects that were already created in the world and can be reused. For instance, if a wood box was added into the world with some specific physical characteristics, a name can be assigned to its USE attribute and then it can be reused instead of creating a new one with same characteristics. The PROTO nodes contains a set of 3D models as robots, objects, vehicles, etc. Fruits, toys, drinks, plants, chairs, stairs and buildings are only a small part of the big set of modeled objects. The robots node offers as well a big diversity of models. From simple robots as the e-puck (figure \ref{fig:ch-2:e-puck}) model which is used very often in research, to more complex robots as the very well known humanoid Nao (figure \ref{fig:ch-2:nao}) are some examples of a total of 44 3D modeled robots that are available to use within the simulator tool. 

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/e-puck.png}
  	\caption{e-puck}
  	\label{fig:ch-2:e-puck}
  \end{subfigure}
  \begin{subfigure}[b]{0.35\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/nao.png}
  	\caption{Nao}
  	\label{fig:ch-2:nao}
  \end{subfigure}
  \caption{Robots}
\end{figure}

Another powerful feature of Webots is to create a custom robot model from scratch using a tree-based structure of solid nodes which are virtual objects with physical properties. Thus a robot is made of a set of solid nodes put them together using joint nodes which are abstract nodes that models mechanical joints. Figure \ref{fig:ch-2:joints} shows the different types of joints that can be used.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.47\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/ballJoint.png}
  	\caption{Ball Joint}
  	\label{fig:ch-2:ball-joint}
  \end{subfigure}
  \vspace{0.00mm}
  \begin{subfigure}[b]{0.47\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/hinge2Joint.png}
  	\caption{Hinge 2 Joint}
  	\label{fig:ch-2:highe-2-joint}
  \end{subfigure}
  \vspace{0.00mm}
  \begin{subfigure}[b]{0.47\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/hingeJoint.png}
  	\caption{Hinge Joint}
  	\label{fig:ch-2:hinge-joint}
  \end{subfigure}
  \vspace{0.00mm}
  \begin{subfigure}[b]{0.47\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/sliderJoint.png}
  	\caption{Slider Joint}
  	\label{fig:ch-2:slider-joint}
  \end{subfigure}
  \caption{Different types of joint nodes}
  \source{Webots documentation}
  \label{fig:ch-2:joints}
\end{figure}


A position sensor can be added into the devices property of a joint node in order to monitor it. In like manner a rotational motor node can be added for actuating it. Thus, putting all together a simplistic version of a custom robot can be created based on the tree information and properties given to the simulator.

\subsection{Sensors}\label{sec:ch-2:sensors}

Sensors allows the robot to sense the environment. Webots has a wide range of generic and commercially available sensors that can be plugged into a custom robot. On one side, the compass, distance sensor and position sensor are some few examples of generic sensors. On the other side, camera, lidar, radar and range finder sensors built by different manufacturers as Hokuyo, Velodyne and Microsoft are offered as subcategories of commercially available sensors. 

\begin{itemize}
\item{The \textbf{compass sensor} can be used to express the angle orientation of the robot over the position of the virtual north as a 1, 2 or 3-axis vector (roll, pitch and yaw angles). The virtual north can be specified in the WorldInfo node by the \verb|northDirection| field. The major fields that can be customized are: the xAxis, yAxis, zAxis, lookup table and resolution field. }
\item{The \textbf{distance sensor} measures the distance between the sensor and an object throughout rays collision with objects. Webots models different types of distance sensors as: generic, infra-red, sonar and laser. All of these has a lookup table, number of rays cast by the sensor, aperture angle, gaussian width and resolution field that can be personalized according to the needs.}
\item{A \textbf{position sensor} can be inserted into the device field of a Joint or a Track. It measures the mechanical joint position. Moreover, the noise and resolution of the sensor can be customized. This sensor is useful for estimating the position of a robot given the current position of a mechanical joint, this technique is known as Odometry.}
\end{itemize}

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/compass.png}
  	\caption{Compass sensor}
  	\label{fig:ch-2:compass}
  \end{subfigure}
  \vspace{0.00mm}
  \begin{subfigure}[b]{0.3\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/distance-sensor.png}
  	\caption{Distance sensor}
  	\label{fig:ch-2:distance-sensor}
  \end{subfigure}
  \vspace{0.00mm}
  \begin{subfigure}[b]{0.3\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/position-sensor.png}
  	\caption{Position sensor}
  	\label{fig:ch-2:position-sensor}
  \end{subfigure}
  \vspace{0.00mm}
  \caption{Sensors}
  \source{Webots documentation}
  \label{fig:ch-2:sensors}
\end{figure}

Sensors have some fields in common. For instance, the resolution field that indicates the sensitivity of the measurement. When it is set to -1 means that it will measure the infinitesimal smallest perceived change. As another example, the lookup table is a field that maps a value from the sensor read data to another custom value. Additionally, the standard deviation noise introduced by the sensor can be specified for a given measure. 

Table \ref{tab:ch-2:lookup} shows the possible values that can be associated with the lookup table field. The first column represents the data measured by the sensor, the second column is the mapped value and the third column represents the noise as a gaussian random number whose range is calculated as a percentage of the response value\cite{cyberbotics}. For instance, for the row $[0.3, 50, 0.1]$ when the sensor senses an object to 0.3m of distance from the robot, it will return a value of $50 \pm 5$, where 5 represents the standard deviation, that is the $10\%$ of 50.
\begin{table}[h!]
\centering
 \begin{tabular}{c c c} 
 \hline
 Value & Mapped value & Noise \\ [0.5ex] 
 \hline\hline
 0 & 1000 & 0 \\ 
 \hline
0.1 & 1000 & 0.1 \\ 
 \hline
 0.2 & 400 & 0.1 \\ 
 \hline
 0.3 & 50 & 0.1 \\ 
 \hline
 0.37 & 30 & 0 \\ 
 \hline
\end{tabular}
\caption{Lookup table of distance sensor}
\source{Webots documentation}
\label{tab:ch-2:lookup}
\end{table}

Figure \ref{fig:ch-2:lookup} shows the relation between the current sensor values and the mapped values within the associated noise.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{\images/chapter2/lookuptable.png}
  \caption{Sensor response versus obstacle distance}
  \source{Webots documentation}
  \label{fig:ch-2:lookup}
\end{figure}

\subsection{Actuators}

Actuators allows the robot to modify the environment. Webots can simulate rotational motors, linear motors, speakers, LEDs, etc. 

\section{Probabilistic Robotics}

Robotics, according to Thrun et al.\cite{Thrun:2005:PR:1121596}, is the science of perceiving and manipulating the physical world through computer-controlled mechanical devices and it is taking place in wide areas such as medicine\cite{Azad:STAR}, planet exploration\cite{Geoffrey:venus}, agriculture\cite{Shamshiri:research-agricultural}, construction\cite{Pileun:construction}, entertainment\cite{Morris:entertainment} among others. 

The robotics that is known nowadays has being evolved across the years. From robotic systems designed to automatize highly repetitive and physically demanding human tasks in the early-to-mid-1940s\cite{Ferreira:prob}, passing through researches making the strong assumption of having exact models of robots and environments in the 1970s, to probabilistic robotics in the mid-1990s\cite{Thrun:robotic-statistics}. Thus a robot needs to deal with uncertainty most of the time. As an example, a robot ordered to walk 1 meter forward from its current position will not end up exactly 1 meter far from its initial position due to some errors, if the robot speed was high and it slipped or the noise produced by the actuators is significant. That is why it needs to be capable to deal with uncertainty, predicting and preserving its current position and orientation within the environment\cite{Nikos:auxiliary-pf}.

\subsection{State}

State, according to Thrun et al.\cite{Thrun:2005:PR:1121596}, refers to the set of all aspects of the robot and its environment that can influence to the future. Two types of state can be defined:

\begin{itemize}
\item \textbf{Dynamic state} changes its position over time. For instance, the people or other robots location.
\item \textbf{Static state} does not change its position over time. For example, the walls or other moveless objects location.
\end{itemize}

Some instances of state are: 

\begin{itemize}
\item The position of physical static entities in the environment as walls, boxes, doors, etc.
\item The location and speed of mobile entities in the environment as people, other robots, etc.
\item The robot pose is usually defined by its position and its orientation relative to a global reference frame. A robot moves on a \textit{fixed frame} which is attached to the ground and does not move. This frame is called the \textit{global reference frame} (GRF). Additionally, a robot is linked within a frame which moves along with the robot. This frame is referred as \textit{local reference frame} (LRF). Communication between the coordinate frames is known as \textit{transformation of frames} and it is a key concept when modeling and programming a robot\cite{Reza:Theory-of-applied-robotics}. Figure \ref{fig:ch-2-global-local} illustrates the difference between GRF and LRF where the $X_R$ axis points to the right side of the robot, the $Y_R$ axis is aligned to its longitudinal axis and the $Z_R$ axis points upward. Besides to these Cartesian coordinates, the robot's angle orientation is defined by the Euler angles: Yaw, Pitch and Roll (see \cite{Cook:mobile-robots}). 
\end{itemize}

The notation used to represent a state at time $t$ will be denoted as $x_t$.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/local-system.png}
  \end{subfigure}
  \caption{Local vs Global reference frame}
  \label{fig:ch-2-global-local}
\end{figure}

\subsection{Robot Versus The Environment}

A robot can interact with the environment in two ways. First, it can modify the environment state using its actuators which are drivers acting as muscles that let the robot change its configuration\cite{Reza:Theory-of-applied-robotics}. For instance, a rotational motor is an actuator that can power a joint and thus changing the robot pose. As another example, a robotic arm can be used to move objects from one position to another. Second, a robot can sense the environment using its sensors which are elements, usually attached to the robot, for obtaining information about internal and environmental states \cite{Reza:Theory-of-applied-robotics}\cite{Thrun:2005:PR:1121596}. 

The control data is information about the change of state in the environment\cite{Thrun:2005:PR:1121596}. It can be, for instance, the velocity or the acceleration of the robot at a given time $t$ and thus it will be represented as $u_t$.  The sensor data provided at time $t$ is denoted as $z_t$ and therefore a sequence of sensor observations will be $z_{1:t}= {z_1, z_2, ..., z_t}$.

\subsection{Belief Representation}\label{ch-2:sub:belief-representation}

A belief is a representation of the internal state of the robot about its position in the environment whereas the true state is where the robot truly is in the environment. In other words it is the probability that a robot at time $t$ is at location $x$ given previous sensor data $z_1, z_2, ..., z_t$ and previous control actions $u_1, u_2, ..., u_t$. A belief can be expressed as the conditional probability function of state $x_t$ given sensor data $z_{1:t} \text{ and control actions } u_{1:t}$. Thus it assigns a probability to each location in the environment, dealing with a single-hypothesis belief (robot tracks a single possible solution) or multiple-hypothesis beliefs (robot tracks an infinite set of positions)\cite{Siegwart:intro-autonumous-robots}. Following Thrun et al. notation, a belief over a state variable $x_t$ will be denoted as $bel(x_t)$ which is the posterior density conditioned on all past measurements and control actions.

\begin{equation}
bel(x_t) = p(x_t | z_{1:t}, u_{1:t})
\label{eq:ch2:bel-update}
\end{equation}

Equation \ref{eq:ch2:bel-posterior} shows the posterior without including the last measurement data. It is usually called $prediction$ in the probabilistic filtering area.

\begin{equation}
\overline{bel}(x_t) = p(x_t | z_{1:t-1}, u_{1:t})
\label{eq:ch2:bel-posterior}
\end{equation}

The probability $bel(x_t)$ can be calculated using the previous posterior belief before measurement $z_t$ and after taking action $u_t$, that is $\overline{bel}(x_t)$. This is known as correction\cite{Thrun:robotic-statistics}. 

The relation between equation \ref{eq:ch2:bel-update} and \ref{eq:ch2:bel-posterior} is shown in appendix \ref{eq:ch-2:bel-derivation}.

\subsection{The Markov Property}

The $Markov Property$ states that a future state depends on the present state only and thus it resumes all the past states information.

Formally, let $x_t$ be a stochastic discrete process that can take on a discrete set of values, where $t \in \mathbb{N}$. A value in process $x_t$ has the \textit{Markov Property} if for all $t$ such that $t\geq1$, the probability distribution of $x_{t+1}$ is determined by the state $x_t$ of the process at time $t$, and does not rely on past values $x_t$ for $t = 0, 1, ..., t - 1$\cite{Privault:markov-chains}. That is:

\begin{equation}
p(x_{t+1} | x_t, x_{t-1}, ..., x_0) = p(x_{t+1} | x_t)
\end{equation}

The Markov Property is an approximation, commonly used in mobile robotics since it simplifies tracking, reasoning and planning and hence it has been found to be very robust for such applications\cite{Siegwart:intro-autonumous-robots}. 


\subsection{Localization Methods}

Navigation is about controlling and operating the course of a robot and it is one of the most challenging skills that a mobile robot needs to master in order to successfully moving through the environment. According to Siegwart et al. what is important for a robot to succeed in navigation is to succeed likewise in these four blocks of navigation:

\begin{itemize}
\item \textit{Sense:} the robot perceives the environment though its sensors and extract meaningful data in order to process and obtain information.
\item \textit{Localization:} the robot knows where it is in the environment.
\item \textit{Cognition:} the robot creates an execution plan in how to act to reach its goals.
\item \textit{Control Action:} the robot executes the execution plan through modulating its output motors.
\end{itemize}

Sensors and actuators take part in determining the robot's localization but both are subjected to noise and thus the problem of localization becomes difficult. Another problem with sensors is that they usually does not provide enough information content to determine the position of the robot and therefore make it easy for the robot to be in an ambiguous location. This problem is known as sensor aliasing and along with sensors and actuators noise turns the localization problem into a difficult task\cite{Siegwart:intro-autonumous-robots}.

There are two types of localization methods categorized by the increased degree of difficulty as stated in Ferreira et al.\cite{Ferreira:prob}: 

\begin{itemize}
\item \textit{Local techniques} (also called position tracking\cite{Thrun:2005:PR:1121596}), when the robot knows its initial position and it is not able to recover it if the robot loses track of it.
\item \textit{Global techniques} (see also \cite{Feng:where-am-I}), when the robot does not have any information about its initial position and it is able to estimate it even in a global uncertainty. This technique includes the \textit{kidnapped robot problem} in which a robot that knows its position is kidnapped and take it into another unknown location and its task is to recover its position. According to Thrun et al.\cite{Thrun:2005:PR:1121596} the later should be categorized as a third localization technique given its difficulty. This technique is more powerful than the former because it deals with global uncertainty and consequently a robot is able to recover its position even if its error is significant.
\end{itemize}

Local techniques approximate the pose using unimodal techniques contrary to global techniques where the robot’s internal knowledge about the state of the environment is represented by a probability density function (pdf) over the space of all locations\cite{Thrun:robotic-statistics}. An example of a global technique is shown in section \ref{sec:ch-2:particles-filter} where a robot handles multiple ambiguous positions. Then after moving, it can sense other factors in the environment that can lead to disambiguate its pose and put most of the probability in a single location and therefore having a better idea of where truly is as image \ref{fig:ch-2:particles-ex} illustrates. A more extensive and explicative example can be found in \cite{Thrun:robotic-statistics} and \cite{Liao:bayesian-filters} where the global localization problem is illustrated in a one-dimension space with a robot that has an hypothetical sensor that senses the presence of doors, when the robot finds the first door, the probability is distributed to all the places where there are doors (see figure \ref{fig:ch-2:sensors}). Thus the probability to be near a door is higher when the sensor detects a door but the robot does not know at this point which door is facing. Moving the robot forward makes the sensor to sense another door and thus increasing the probability of being at the second door. This example illustrates some important properties of the Bayes Filter.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/first-door.png}
  \end{subfigure}
  \vspace{1cm}
  \begin{subfigure}[b]{\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/second-door.png}
  \end{subfigure}
  \caption{Probability density when robot senses.}
  \source{Extracted from Thrun et al. Probabilistic Robotics\cite{Thrun:robotic-statistics}}
  \label{fig:ch-2:bayes-filter-example}
\end{figure}


\subsection{Bayes Filter}\label{sec:ch-2:bayes-filter}

As claimed by Borriello et al.\cite{Liao:bayesian-filters} Bayes Filter is a statistically estimator of dynamic system's states based on noisy observations. In other words it is an algorithm that calculates beliefs distribution based on measurements and control data\cite{Thrun:2005:PR:1121596}. This is generally done in two steps as it was mentioned in section \ref{ch-2:sub:belief-representation}: the prediction and the correction step and thus each time a robot receives the sensors measurement data the robot controller software needs to compute the posterior densities shown in equation \ref{eq:ch2:bel-posterior} but notice that such task is computationally heavy and consequently its time complexity grows exponentially because the amount of sensors measurements increasing over time. A solution to this problem is to assume that such dynamic system has the Markovian Property. That is the future state $x_{t+1}$ depends on the present state $x_t$ because the assumption that $x_t$ carries all the needed information is made.

Bayes filter algorithm is recursive. It needs the previous posterior distribution to calculate the current posterior distribution. The algorithm is described below and its correctness is shown in appendix \ref{sec:ap:bayes-filter}.

\IncMargin{1em}
\begin{algorithm}

\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$bel(x_{t-1})$, $u_t$, $z_t$}
\Output{$bel(x_t)$}
\BlankLine

  \ForEach{$x_t$}{
    $\overline{bel}(x_t)\leftarrow \int p(x_t | u_t, x_{t-1}) \: bel(x_{t-1}) dx_{t-1}$\\
    ${bel}(x_t)\leftarrow \eta \: p(z_t | x_t) \: \overline{bel}(x_t)$\\
  }
  \Return $bel(x_t)$
\caption{Bayes Filtering Algorithm\cite{Thrun:2005:PR:1121596}}

\label{ch-2:algo:bayes-filter}
\end{algorithm}\DecMargin{1em}

There are many algorithms that apply Bayes Filtering. Some examples are stated below.
\begin{itemize}
\item The \textit{Kalman Filter} (KF) is a Gaussian technique invented in 1950 by Rudolph Emil Kalman\cite{Kalman:filter} and was first implemented by NASA in the Apollo program to estimate the trajectory of the space capsule in real time\cite{Barrau:IKF}. It takes noisy data and put the noise apart in order to get information with less uncertainty\cite{Matthew:kalman-tutorial}. It works with continuous states and it represents the beliefs by the first and second moment\cite{Liao:bayesian-filters} from multivariate normal distributions. Chen et al. have proposed another version of KF called Maximum Correntropy Kalman Filter (MCKF) which is an effective and robust algorithm for non-Gaussian signals and heavy-tailed impulsive noise. Instead of using the Minimum Mean Square Error (MMSE) as KF, its optimality criterion is the Maximum Correntropy Criterion (MCC)\cite{Chen:MCKF}. 
\item The \textit{Extended Kalman Filter} (EKF) does not assume any linearity as KF does and thus the next state and the measurement probabilities are nonlinear functions\cite{Zarchan:fundam-kalman}\cite{Thrun:2005:PR:1121596}. Some variants of this method include Unscented Kalman Filter, where the state distribution is approximated by a Gaussian random variable as in EKF, but now a minimal set of sample points are cautiously chosen for representing the state distribution\cite{Julier:UKF}\cite{Wan:UKF}. Another improvement of EKF is the Invariant Extended Kalman Filter (IEKF) used for continuous-time systems with discrete observations\cite{Barrau:IEKF}.
\item The \textit{Grid-based Filters} split the environment into small grids, each containing a belief of the true position state\cite{Liao:bayesian-filters}. For instance, Burgard et al. proposed a bayesian approach based on certainty grids where each cell has associated a probability of the robot to be on that cell and thus they successfully predicted the robot absolute position and orientation in a global localization problem using standard sensors in complex environments\cite{Burgard:grid-based}.
\item \textit{Topological} approaches represent the environment using graphs where each node is a representation of a location and the edges are the connectivity between two locations\cite{Liao:bayesian-filters}.
\item The \textit{Particles Filter} (PF) according to Rekleitis, uses many copies of the variable of interest, each of them is associated with a weight representing the quality of that specific particle. Each particle executes the same control action plus some noise, they are reevaluated, its weight is recalculated and therefore the particles with low weight are eliminated in a process called resampling\cite{Rekleitis:particles-filter}.
\end{itemize}

\subsection{Particles Filter}\label{sec:ch-2:particles-filter}

Particles Filter is a nonparametric implementation of the Bayes Filter algorithm where the posterior is approximated by a set of $M$ samples called \textit{particles} where $M$ is usually a large number(e.g. 2000). Under the context of localization, it is also known as Monte Carlo localization (MCL)\cite{Thrun00j}. Each particle has associated a weight that represents the contribution to the overall estimate of the posterior\cite{Rekleitis:particles-filter}. Thus equation \ref{eq:ch-2:particles}  shows the believe at time $t$ approximated by a set of particles and weights of $M$ particles.

\begin{equation}\label{eq:ch-2:particles}
bel(x_t) \approx S_t = \lbrace \langle x_t^{[m]},w_t^{[m]} \rangle |  1 \leq m \leq M \rbrace
\end{equation}

Where $x$ denotes the state of particle $m$, and $w$ represents the weight of such particle. 

According to Thrun et al. the probability of including a particle $x_t^{[m]}$ will be proportional to its posterior belief (see equation \ref{eq:ch-2:particles-proportion}) and thus more particles in a region of the environment means that the true state is more likely to be in that region.

\begin{equation}\label{eq:ch-2:particles-proportion}
x_t^{[m]} \sim p(x_t | z_{1:t}, u_{1:t})
\end{equation}

Table \ref{ch-2:algo:particles-filter} illustrates the Particles Filter algorithm. The algorithm has two loops, representing the prediction and resampling steps respectively. The prediction step obtains M hypothesis about the true state using sampling techniques. All of these hypothesis together correspond to $\overline{bel}(x_t)$ in the Bayes algorithm. Furthermore the weight or importance factor for each particle is calculated as a conditional probability of sensing $z_t$ given the state of that specific particle representing the posterior $bel(x_t)$ in the Bayes algorithm. The resampling step consists in selecting with replacement M particles from the temporal data set $\bar{S_t}$ with probability proportional to each particles' weight and thus the particles with lower weights have less probability to be selected\cite{Thrun:2005:PR:1121596}. Figure \ref{fig:ch-2:particles-ex} illustrates the use of MCL for robot localization.

\IncMargin{1em}
\begin{algorithm}

\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$S_{t-1}$, $u_t$, $z_t$}
\Output{$S_t$}
\BlankLine
$\bar{S_t} \leftarrow \emptyset$\\
$S_t \leftarrow \emptyset$\\
  \For{$m = 1$ to $M$}{
    sample $x_t^{[m]}\backsim p(x_t | u_t, x_{t-1}^{[m]})$ \\
    $w_t^{[m]} \leftarrow p(z_t | x_t^{[m]}) $\\
    $\bar{S_t} \leftarrow \bar{S_t} \cup \lbrace \langle x_t^{[m]},w_t^{[m]} \rangle \rbrace$\\
  }
  \For{$m = 1$ to $M$}{
    $\langle x_t, w_t \rangle \leftarrow \text{draw } i \text{ from } \bar{S_t} \text{ with probability } \propto w_t^{[i]}$\\
    ${S_t} \leftarrow {S_t} \cup \lbrace \langle x_t, w_t \rangle \rbrace$\\
  }
  \Return $S_t$
\caption{Particles Filtering Algorithm. Adapted from \cite{Thrun:2005:PR:1121596}.}

\label{ch-2:algo:particles-filter}
\end{algorithm}\DecMargin{1em}

Rekleitis et al. mention other resampling techniques: Linear Time Resampling and Resampling by Liu et al. The former uses the method of simulating order statistics\cite{Gerontidis:stats}, that is, obtaining $M+1$ random numbers, applying the negative log number and then calculating their cumulative sum, then compare them to the cumulative sum over the weights in order to generate a list of indexes corresponding to the particles with higher weights\cite{Carpenter:improved-pf}. Resampling by Liu et al. proposed the use of a function of the weights ($a_j=f(w_j)$), for instance the square root $f(w_j) = \sqrt w_j$ and then normalize this quantity regarding the number of particles M. Then if $a_j$ is greater or equal to one, the $jth$ particle is duplicated $k$ times, where $k=\lfloor a_j \rfloor$; otherwise the particle is selected with a probability equal to $a_j$\cite{Liu:resampling}\cite{Rekleitis:particles-filter}.

Zhou et al. proposed an improved PF algorithm called the Pearson Particles Filter (PPF) because it is based on the Pearson Correlation Coefficient (PCC) which is a statistical technique to determine the linear dependence between two random variables and thus it is used to decide how close the hypothetical particle state is to the true state value. Even though this technique solves the degeneracy and sample impoverishment problem present in PF, it is computationally complex\cite{Zhou:sampling-method}. 

Murray et al. present an alternative to accelerate the resampling technique eliminating the cumulative sum over weights using two schemes based on Metropolis\cite{Metropolis:fast-computing} and rejection samplers making the problem of resampling parallelizable using graphical processing units (GPU)\cite{Murray:parallel-res}. Nicely et al. go even further, introducing a general purpose graphical processing units (GPGPUs) implementation which takes advantage of the properties of the cumulative sum and the evolutionary nature of the particle weighting process to parallelize the re-index section of the algorithm\cite{Nicely:improvised-parallel-res}.


\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.65\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/particles-1.png}
	\caption{}
  	\label{fig:ch-2:particles-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.47\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/particles-2.png}
	\caption{}
  	\label{fig:ch-2:particles-2}
  \end{subfigure}
  \vspace{0.00mm}
  \begin{subfigure}[b]{0.47\linewidth}
  	\includegraphics[width=\linewidth]{\images/chapter2/particles-3.png}
	\caption{}
  	\label{fig:ch-2:particles-3}
  \end{subfigure}
  \vspace{0.00mm}
  \caption[A robot localization example using PF.]{A robot localization example using PF. (\subref{fig:ch-2:particles-1}) The robot has a global uncertainty about its position in the map, (\subref{fig:ch-2:particles-2}) it handles some hypothesis about where it is after executing certain control actions, (\subref{fig:ch-2:particles-3}) it is quite sure where it is in the environment.}
  \source{Particles Filters in Robotics\cite{Thrun:particle-filter-robotics}.}
  \label{fig:ch-2:particles-ex}
\end{figure}

As claimed by Thrun, one drawback of PF is that the performance of the algorithm highly depends on the environment dimension, that is, higher the dimension, worst its performance is. For instance, simultaneous localization and mapping problem (SLAM) is a clear example in which the robot does not have a previous built map, deals with a high-dimensional environment and does not have any information about its initial position. Consequently, SLAM cannot be resolved using conventional MCL algorithms due to the absence of an initial map. Doucet et al. find a solution to this problem with a variation of PF called Rao-Blackwellised Particle Filter resolving the problem efficiently up until 100000 dimensions\cite{Doucet:2000:RPF:2073946.2073968}. Montemerlo et al. proposed FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations\cite{Montemerlo02a} and it is one of the most robust algorithms for mapping in-door environments\cite{Thrun:particle-filter-robotics}.













